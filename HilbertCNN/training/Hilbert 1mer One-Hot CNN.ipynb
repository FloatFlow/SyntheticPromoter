{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Adaptation of the Hilbert CNN at https://openreview.net/forum?id=HJvvRoe0W\n",
    "Works by encoding each nucleotide as a one-hot vector, \n",
    "then fits it to a image-like grid using a hilbert curve\n",
    "such that each 'pixel' is a 1mer of length 4\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, BatchNormalization, AveragePooling2D, Dense, Dropout, SeparableConv2D, Add\n",
    "from keras.layers import Activation, Input, Concatenate, Flatten, MaxPooling2D, Reshape, GaussianNoise\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "import image\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17588 images belonging to 2 classes.\n",
      "Found 1955 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "start_target_size = (32, 32, 4)\n",
    "batch_size = 16\n",
    "train_path = 'D:/Projects/iSynPro/iSynPro/HilbertCNN/train_val_npys/6count/1mer/train'\n",
    "test_path = 'D:/Projects/iSynPro/iSynPro/HilbertCNN/train_val_npys/6count/1mer/test'\n",
    "\n",
    "# define generators\n",
    "train_datagen = image.ImageDataGenerator()\n",
    "test_datagen = image.ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_np_from_directory(train_path, \n",
    "                                                    target_size= start_target_size, \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    class_mode='binary',\n",
    "                                                    seed=42) \n",
    "\n",
    "validation_generator = test_datagen.flow_np_from_directory(test_path, \n",
    "                                                        target_size= start_target_size, \n",
    "                                                        batch_size=batch_size, \n",
    "                                                        class_mode='binary',\n",
    "                                                        seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# original implementation of Hilbert-CNN\n",
    "# https://openreview.net/forum?id=HJvvRoe0W\n",
    "\n",
    "def computation_block(in_layer, n_filters, filtersize_a, filtersize_b, filtersize_c, filtersize_d):\n",
    "\n",
    "    # residual 1\n",
    "    p1 = Conv2D(n_filters, (filtersize_a, filtersize_a), strides=(1, 1), padding='same')(in_layer)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "    p1 = Activation('relu')(p1)\n",
    "    p1 = Conv2D(n_filters, (filtersize_b, filtersize_b), strides=(1, 1), padding='same')(p1)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "\n",
    "    # residual 2\n",
    "    p2 = Conv2D(n_filters, (filtersize_c, filtersize_c), strides=(1, 1), padding='same')(in_layer)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "    p2 = Activation('relu')(p2)\n",
    "    p2 = Conv2D(n_filters, (filtersize_d, filtersize_d), strides=(1, 1), padding='same')(p2)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "\n",
    "    x = Concatenate()([in_layer, p1, p2])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# stem\n",
    "inputs = Input(shape=[32, 32, 4])\n",
    "x = GaussianNoise(0.2)(inputs)\n",
    "x = Conv2D(64, (7, 7), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (5, 5), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = computation_block(x, 4, 8, 4, 4, 3)\n",
    "x = computation_block(x, 4, 3, 3, 3, 3)\n",
    "# mid-stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = computation_block(x, 4, 2, 4, 4, 3)\n",
    "x = computation_block(x, 4, 2, 2, 2, 2)\n",
    "x = computation_block(x, 4, 3, 2, 2, 3)\n",
    "\n",
    "\n",
    "# exit stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x) #we omit this last avgpool to retain dimensionality\n",
    "x = Flatten()(x)\n",
    "\n",
    "# FC layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer= SGD(lr= 0.01, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 32, 32, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_9 (GaussianNoise (None, 32, 32, 4)    0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 32, 32, 64)   12608       gaussian_noise_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 32, 32, 64)   256         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 32, 32, 64)   102464      batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 32, 32, 64)   256         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 64)   0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, 16, 16, 64)   0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 16, 16, 4)    16388       average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 16, 16, 4)    4100        average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 16, 16, 4)    16          conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 16, 16, 4)    16          conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 4)    0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 4)    0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 16, 16, 4)    260         activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 16, 16, 4)    148         activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 16, 16, 4)    16          conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 16, 16, 4)    16          conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 16, 16, 72)   0           average_pooling2d_13[0][0]       \n",
      "                                                                 batch_normalization_166[0][0]    \n",
      "                                                                 batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 16, 16, 72)   288         concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 72)   0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 16, 16, 4)    2596        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 16, 16, 4)    2596        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 16, 16, 4)    16          conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 16, 16, 4)    16          conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 4)    0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 16, 16, 4)    0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 16, 16, 4)    148         activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 16, 16, 4)    148         activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 16, 16, 4)    16          conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 16, 16, 4)    16          conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 16, 16, 80)   0           activation_30[0][0]              \n",
      "                                                                 batch_normalization_171[0][0]    \n",
      "                                                                 batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 16, 16, 80)   320         concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 80)   0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 8, 8, 80)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 8, 8, 4)      1284        average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 8, 8, 4)      5124        average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 8, 8, 4)      16          conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 8, 8, 4)      16          conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 4)      0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 4)      0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 8, 8, 4)      260         activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 8, 8, 4)      148         activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 8, 8, 4)      16          conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 8, 8, 4)      16          conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 8, 8, 88)     0           average_pooling2d_14[0][0]       \n",
      "                                                                 batch_normalization_176[0][0]    \n",
      "                                                                 batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 8, 8, 88)     352         concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 88)     0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 8, 8, 4)      1412        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 8, 8, 4)      1412        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 8, 8, 4)      16          conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 8, 8, 4)      16          conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 4)      0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 4)      0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 8, 8, 4)      68          activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 8, 8, 4)      68          activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 8, 8, 4)      16          conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 8, 8, 4)      16          conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 8, 8, 96)     0           activation_36[0][0]              \n",
      "                                                                 batch_normalization_181[0][0]    \n",
      "                                                                 batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 8, 8, 96)     384         concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 96)     0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 8, 8, 4)      3460        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 8, 8, 4)      1540        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 8, 8, 4)      16          conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 8, 8, 4)      16          conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 4)      0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 8, 8, 4)      0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 8, 8, 4)      68          activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 8, 8, 4)      148         activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 8, 8, 4)      16          conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 8, 8, 4)      16          conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 8, 8, 104)    0           activation_39[0][0]              \n",
      "                                                                 batch_normalization_186[0][0]    \n",
      "                                                                 batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 8, 8, 104)    416         concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 8, 8, 104)    0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_15 (AveragePo (None, 4, 4, 104)    0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 4, 4, 104)    416         average_pooling2d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 104)    0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, 2, 2, 104)    0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 416)          0           average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1024)         427008      flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1024)         1049600     dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1)            1025        dense_18[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,637,089\n",
      "Trainable params: 1,635,585\n",
      "Non-trainable params: 1,504\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53822, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-01-0.54.hdf5\n",
      " - 48s - loss: 0.5807 - binary_accuracy: 0.6855 - val_loss: 0.5382 - val_binary_accuracy: 0.7339\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 35s - loss: 0.5468 - binary_accuracy: 0.7177 - val_loss: 0.5489 - val_binary_accuracy: 0.7091\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53822 to 0.53342, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-03-0.53.hdf5\n",
      " - 35s - loss: 0.5308 - binary_accuracy: 0.7234 - val_loss: 0.5334 - val_binary_accuracy: 0.7081\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.53342 to 0.53088, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-04-0.53.hdf5\n",
      " - 36s - loss: 0.5225 - binary_accuracy: 0.7317 - val_loss: 0.5309 - val_binary_accuracy: 0.7091\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.53088 to 0.52385, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-05-0.52.hdf5\n",
      " - 36s - loss: 0.5142 - binary_accuracy: 0.7370 - val_loss: 0.5238 - val_binary_accuracy: 0.7143\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.52385 to 0.50633, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-06-0.51.hdf5\n",
      " - 35s - loss: 0.5105 - binary_accuracy: 0.7355 - val_loss: 0.5063 - val_binary_accuracy: 0.7308\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 35s - loss: 0.5056 - binary_accuracy: 0.7457 - val_loss: 0.5150 - val_binary_accuracy: 0.7210\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 35s - loss: 0.4981 - binary_accuracy: 0.7504 - val_loss: 0.5087 - val_binary_accuracy: 0.7277\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50633 to 0.50555, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-09-0.51.hdf5\n",
      " - 36s - loss: 0.5002 - binary_accuracy: 0.7457 - val_loss: 0.5055 - val_binary_accuracy: 0.7313\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 35s - loss: 0.4960 - binary_accuracy: 0.7473 - val_loss: 0.5150 - val_binary_accuracy: 0.7287\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50555 to 0.50195, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-11-0.50.hdf5\n",
      " - 35s - loss: 0.4930 - binary_accuracy: 0.7522 - val_loss: 0.5019 - val_binary_accuracy: 0.7272\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.50195 to 0.49894, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-12-0.50.hdf5\n",
      " - 35s - loss: 0.4910 - binary_accuracy: 0.7522 - val_loss: 0.4989 - val_binary_accuracy: 0.7411\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 35s - loss: 0.4787 - binary_accuracy: 0.7607 - val_loss: 0.5066 - val_binary_accuracy: 0.7318\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 35s - loss: 0.4899 - binary_accuracy: 0.7524 - val_loss: 0.5015 - val_binary_accuracy: 0.7251\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 35s - loss: 0.4839 - binary_accuracy: 0.7575 - val_loss: 0.5099 - val_binary_accuracy: 0.7303\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.49894 to 0.49594, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-16-0.50.hdf5\n",
      " - 35s - loss: 0.4806 - binary_accuracy: 0.7593 - val_loss: 0.4959 - val_binary_accuracy: 0.7421\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 35s - loss: 0.4797 - binary_accuracy: 0.7604 - val_loss: 0.5256 - val_binary_accuracy: 0.7148\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.49594 to 0.49571, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-18-0.50.hdf5\n",
      " - 36s - loss: 0.4782 - binary_accuracy: 0.7590 - val_loss: 0.4957 - val_binary_accuracy: 0.7463\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 35s - loss: 0.4717 - binary_accuracy: 0.7644 - val_loss: 0.5092 - val_binary_accuracy: 0.7261\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 35s - loss: 0.4747 - binary_accuracy: 0.7671 - val_loss: 0.5115 - val_binary_accuracy: 0.7225\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 35s - loss: 0.4746 - binary_accuracy: 0.7644 - val_loss: 0.5205 - val_binary_accuracy: 0.7179\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 35s - loss: 0.4737 - binary_accuracy: 0.7630 - val_loss: 0.5138 - val_binary_accuracy: 0.7287\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 35s - loss: 0.4734 - binary_accuracy: 0.7643 - val_loss: 0.4975 - val_binary_accuracy: 0.7365\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 35s - loss: 0.4705 - binary_accuracy: 0.7634 - val_loss: 0.5306 - val_binary_accuracy: 0.7189\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 35s - loss: 0.4638 - binary_accuracy: 0.7705 - val_loss: 0.5142 - val_binary_accuracy: 0.7292\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 35s - loss: 0.4717 - binary_accuracy: 0.7668 - val_loss: 0.5184 - val_binary_accuracy: 0.7272\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 35s - loss: 0.4680 - binary_accuracy: 0.7665 - val_loss: 0.5061 - val_binary_accuracy: 0.7298\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 35s - loss: 0.4665 - binary_accuracy: 0.7662 - val_loss: 0.5168 - val_binary_accuracy: 0.7298\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 35s - loss: 0.4696 - binary_accuracy: 0.7666 - val_loss: 0.5116 - val_binary_accuracy: 0.7349\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 35s - loss: 0.4653 - binary_accuracy: 0.7687 - val_loss: 0.5044 - val_binary_accuracy: 0.7380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1876eb3d4e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "lr_descent = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                               factor=0.5,\n",
    "                                               patience=5,\n",
    "                                               verbose=1,\n",
    "                                               mode='auto',\n",
    "                                               epsilon=0.0001,\n",
    "                                               cooldown=1,\n",
    "                                               min_lr=0)\n",
    "\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "#tracking = keras.callbacks.ProgbarLogger(count_mode='samples')\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modified version of Hilbert-CNN\n",
    "# batchnorm after activation\n",
    "\n",
    "def computation_block(in_layer, n_filters, filtersize_a, filtersize_b, filtersize_c, filtersize_d):\n",
    "\n",
    "    # residual 1\n",
    "    p1 = Conv2D(n_filters, (filtersize_a, filtersize_a), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "    p1 = Conv2D(n_filters, (filtersize_b, filtersize_b), strides=(1, 1), padding='same', activation='relu')(p1)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "\n",
    "    # residual 2\n",
    "    p2 = Conv2D(n_filters, (filtersize_c, filtersize_c), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "    p2 = Conv2D(n_filters, (filtersize_d, filtersize_d), strides=(1, 1), padding='same', activation='relu')(p2)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "\n",
    "    x = Concatenate()([in_layer, p1, p2])\n",
    "    #x = Activation('relu')(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "# stem\n",
    "inputs = Input(shape=[32, 32, 4])\n",
    "x = GaussianNoise(0.3)(inputs)\n",
    "x = Conv2D(64, (7, 7), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (5, 5), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = computation_block(x, 4, 8, 4, 4, 3)\n",
    "x = computation_block(x, 4, 3, 3, 3, 3)\n",
    "# mid-stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = computation_block(x, 4, 2, 4, 4, 3)\n",
    "x = computation_block(x, 4, 2, 2, 2, 2)\n",
    "x = computation_block(x, 4, 3, 2, 2, 3)\n",
    "\n",
    "\n",
    "# exit stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "#x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x) #we omit this last avgpool to retain dimensionality\n",
    "x = Flatten()(x)\n",
    "\n",
    "# FC layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer= SGD(lr= 1e-2, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73496, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-01-0.73.hdf5\n",
      " - 404s - loss: 0.6319 - binary_accuracy: 0.6322 - val_loss: 0.7350 - val_binary_accuracy: 0.5881\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73496 to 0.59861, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-02-0.60.hdf5\n",
      " - 32s - loss: 0.5682 - binary_accuracy: 0.6992 - val_loss: 0.5986 - val_binary_accuracy: 0.6674\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59861 to 0.58810, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-03-0.59.hdf5\n",
      " - 32s - loss: 0.5492 - binary_accuracy: 0.7133 - val_loss: 0.5881 - val_binary_accuracy: 0.7081\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58810 to 0.52329, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-04-0.52.hdf5\n",
      " - 32s - loss: 0.5391 - binary_accuracy: 0.7194 - val_loss: 0.5233 - val_binary_accuracy: 0.7380\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 32s - loss: 0.5277 - binary_accuracy: 0.7275 - val_loss: 0.5254 - val_binary_accuracy: 0.7256\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 32s - loss: 0.5262 - binary_accuracy: 0.7300 - val_loss: 0.5443 - val_binary_accuracy: 0.7215\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.52329 to 0.50540, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-07-0.51.hdf5\n",
      " - 32s - loss: 0.5191 - binary_accuracy: 0.7359 - val_loss: 0.5054 - val_binary_accuracy: 0.7447\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 32s - loss: 0.5165 - binary_accuracy: 0.7341 - val_loss: 0.6366 - val_binary_accuracy: 0.6735\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 32s - loss: 0.5112 - binary_accuracy: 0.7379 - val_loss: 0.5247 - val_binary_accuracy: 0.7282\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 32s - loss: 0.5124 - binary_accuracy: 0.7387 - val_loss: 0.5198 - val_binary_accuracy: 0.7432\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 32s - loss: 0.5064 - binary_accuracy: 0.7407 - val_loss: 0.5158 - val_binary_accuracy: 0.7447\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 32s - loss: 0.5034 - binary_accuracy: 0.7443 - val_loss: 0.5297 - val_binary_accuracy: 0.7287\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 32s - loss: 0.5050 - binary_accuracy: 0.7436 - val_loss: 0.5254 - val_binary_accuracy: 0.7339\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 32s - loss: 0.5002 - binary_accuracy: 0.7498 - val_loss: 0.5253 - val_binary_accuracy: 0.7396\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 32s - loss: 0.4999 - binary_accuracy: 0.7486 - val_loss: 0.5254 - val_binary_accuracy: 0.7308\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.50540 to 0.49668, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-16-0.50.hdf5\n",
      " - 32s - loss: 0.4976 - binary_accuracy: 0.7493 - val_loss: 0.4967 - val_binary_accuracy: 0.7416\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 32s - loss: 0.4960 - binary_accuracy: 0.7485 - val_loss: 0.5207 - val_binary_accuracy: 0.7272\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 32s - loss: 0.4965 - binary_accuracy: 0.7481 - val_loss: 0.5145 - val_binary_accuracy: 0.7282\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 32s - loss: 0.4950 - binary_accuracy: 0.7501 - val_loss: 0.5080 - val_binary_accuracy: 0.7344\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 32s - loss: 0.4946 - binary_accuracy: 0.7518 - val_loss: 0.5388 - val_binary_accuracy: 0.7272\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 32s - loss: 0.4902 - binary_accuracy: 0.7539 - val_loss: 0.5051 - val_binary_accuracy: 0.7339\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 32s - loss: 0.4930 - binary_accuracy: 0.7488 - val_loss: 0.5223 - val_binary_accuracy: 0.7272\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 32s - loss: 0.4903 - binary_accuracy: 0.7548 - val_loss: 0.5169 - val_binary_accuracy: 0.7339\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 32s - loss: 0.4874 - binary_accuracy: 0.7562 - val_loss: 0.5146 - val_binary_accuracy: 0.7365\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 32s - loss: 0.4899 - binary_accuracy: 0.7554 - val_loss: 0.5129 - val_binary_accuracy: 0.7437\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 32s - loss: 0.4887 - binary_accuracy: 0.7544 - val_loss: 0.5139 - val_binary_accuracy: 0.7359\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 32s - loss: 0.4846 - binary_accuracy: 0.7568 - val_loss: 0.5280 - val_binary_accuracy: 0.7236\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 32s - loss: 0.4866 - binary_accuracy: 0.7564 - val_loss: 0.4991 - val_binary_accuracy: 0.7396\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 31s - loss: 0.4843 - binary_accuracy: 0.7574 - val_loss: 0.5169 - val_binary_accuracy: 0.7267\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 32s - loss: 0.4843 - binary_accuracy: 0.7540 - val_loss: 0.5140 - val_binary_accuracy: 0.7359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18741cdd470>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNoise (None, 32, 32, 4)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 64)   12608       gaussian_noise_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 64)   102464      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 4)    16388       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 4)    4100        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 4)    16          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 4)    16          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 4)    260         batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 4)    148         batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 4)    16          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 4)    16          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 72)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 72)   0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 72)   288         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 4)    2596        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 4)    2596        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 4)    16          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 4)    16          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 4)    148         batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 4)    148         batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 4)    16          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 4)    16          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 80)   0           batch_normalization_7[0][0]      \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 80)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 80)   320         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 80)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 4)      1284        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 4)      5124        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 4)      16          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 4)      16          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 4)      260         batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 4)      148         batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 4)      16          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 4)      16          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8, 8, 88)     0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 8, 88)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 88)     352         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 4)      1412        batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 4)      1412        batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 4)      16          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 4)      16          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 4)      68          batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 4)      68          batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 4)      16          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 4)      16          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 8, 8, 96)     0           batch_normalization_17[0][0]     \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "                                                                 batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 8, 8, 96)     0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 96)     384         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 4)      3460        batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 4)      1540        batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 4)      16          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 4)      16          conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 4)      68          batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 4)      148         batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 4)      16          conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 4)      16          conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 8, 8, 104)    0           batch_normalization_22[0][0]     \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 8, 104)    0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 104)    416         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 104)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 4, 104)    416         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 4, 4, 104)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 2, 2, 104)    0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 416)          0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         427008      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1025        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 587,489\n",
      "Trainable params: 585,985\n",
      "Non-trainable params: 1,504\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modified version of Hilbert-CNN\n",
    "# batchnorm after activation\n",
    "\n",
    "def computation_block(in_layer, n_filters, filtersize_a, filtersize_b, filtersize_c, filtersize_d):\n",
    "\n",
    "    # residual 1\n",
    "    p1 = Conv2D(n_filters, (filtersize_a, filtersize_a), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "    p1 = Conv2D(n_filters, (filtersize_b, filtersize_b), strides=(1, 1), padding='same')(p1)\n",
    "    #p1 = BatchNormalization()(p1)\n",
    "\n",
    "    # residual 2\n",
    "    p2 = Conv2D(n_filters, (filtersize_c, filtersize_c), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "    p2 = Conv2D(n_filters, (filtersize_d, filtersize_d), strides=(1, 1), padding='same')(p2)\n",
    "    #p2 = BatchNormalization()(p2)\n",
    "\n",
    "    x = Concatenate()([in_layer, p1, p2])\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "# stem\n",
    "inputs = Input(shape=[32, 32, 4])\n",
    "x = GaussianNoise(0.3)(inputs)\n",
    "x = Conv2D(64, (7, 7), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (5, 5), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = computation_block(x, 4, 8, 4, 4, 3)\n",
    "x = computation_block(x, 4, 3, 3, 3, 3)\n",
    "# mid-stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = computation_block(x, 4, 2, 4, 4, 3)\n",
    "x = computation_block(x, 4, 2, 2, 2, 2)\n",
    "x = computation_block(x, 4, 3, 2, 2, 3)\n",
    "\n",
    "\n",
    "# exit stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Activation('relu')(x)\n",
    "#x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x) #we omit this last avgpool to retain dimensionality\n",
    "x = Flatten()(x)\n",
    "\n",
    "# FC layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer= SGD(lr= 1e-2, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59563, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_close_hilbertcnn/weights-01-0.60.hdf5\n",
      " - 38s - loss: 0.6171 - binary_accuracy: 0.6597 - val_loss: 0.5956 - val_binary_accuracy: 0.6973\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59563 to 0.53943, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_close_hilbertcnn/weights-02-0.54.hdf5\n",
      " - 31s - loss: 0.5720 - binary_accuracy: 0.6952 - val_loss: 0.5394 - val_binary_accuracy: 0.7241\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 30s - loss: 0.5519 - binary_accuracy: 0.7106 - val_loss: 0.6027 - val_binary_accuracy: 0.6906\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.53943 to 0.51902, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_close_hilbertcnn/weights-04-0.52.hdf5\n",
      " - 31s - loss: 0.5408 - binary_accuracy: 0.7163 - val_loss: 0.5190 - val_binary_accuracy: 0.7329\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 30s - loss: 0.5301 - binary_accuracy: 0.7269 - val_loss: 0.6306 - val_binary_accuracy: 0.6663\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 30s - loss: 0.5269 - binary_accuracy: 0.7281 - val_loss: 0.5373 - val_binary_accuracy: 0.7127\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51902 to 0.50467, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_close_hilbertcnn/weights-07-0.50.hdf5\n",
      " - 31s - loss: 0.5210 - binary_accuracy: 0.7357 - val_loss: 0.5047 - val_binary_accuracy: 0.7561\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 30s - loss: 0.5190 - binary_accuracy: 0.7316 - val_loss: 0.5325 - val_binary_accuracy: 0.7478\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 31s - loss: 0.5149 - binary_accuracy: 0.7367 - val_loss: 0.5425 - val_binary_accuracy: 0.7447\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 30s - loss: 0.5090 - binary_accuracy: 0.7412 - val_loss: 0.5165 - val_binary_accuracy: 0.7437\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 30s - loss: 0.5084 - binary_accuracy: 0.7395 - val_loss: 0.5360 - val_binary_accuracy: 0.7344\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 31s - loss: 0.5062 - binary_accuracy: 0.7428 - val_loss: 0.5371 - val_binary_accuracy: 0.7380\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 31s - loss: 0.5045 - binary_accuracy: 0.7457 - val_loss: 0.5752 - val_binary_accuracy: 0.7096\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 31s - loss: 0.5011 - binary_accuracy: 0.7457 - val_loss: 0.5250 - val_binary_accuracy: 0.7380\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 31s - loss: 0.5003 - binary_accuracy: 0.7449 - val_loss: 0.5443 - val_binary_accuracy: 0.7421\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 30s - loss: 0.4997 - binary_accuracy: 0.7451 - val_loss: 0.5278 - val_binary_accuracy: 0.7401\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 30s - loss: 0.4997 - binary_accuracy: 0.7470 - val_loss: 0.5310 - val_binary_accuracy: 0.7329\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 30s - loss: 0.4987 - binary_accuracy: 0.7485 - val_loss: 0.5390 - val_binary_accuracy: 0.7473\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 30s - loss: 0.4984 - binary_accuracy: 0.7497 - val_loss: 0.5470 - val_binary_accuracy: 0.7390\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 30s - loss: 0.4944 - binary_accuracy: 0.7496 - val_loss: 0.5353 - val_binary_accuracy: 0.7473\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 30s - loss: 0.4939 - binary_accuracy: 0.7512 - val_loss: 0.5463 - val_binary_accuracy: 0.7365\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 31s - loss: 0.4922 - binary_accuracy: 0.7509 - val_loss: 0.5544 - val_binary_accuracy: 0.7354\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 30s - loss: 0.4900 - binary_accuracy: 0.7560 - val_loss: 0.5296 - val_binary_accuracy: 0.7488\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 31s - loss: 0.4923 - binary_accuracy: 0.7549 - val_loss: 0.5351 - val_binary_accuracy: 0.7318\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 30s - loss: 0.4909 - binary_accuracy: 0.7513 - val_loss: 0.5315 - val_binary_accuracy: 0.7540\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 31s - loss: 0.4904 - binary_accuracy: 0.7557 - val_loss: 0.5482 - val_binary_accuracy: 0.7287\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 30s - loss: 0.4892 - binary_accuracy: 0.7505 - val_loss: 0.5240 - val_binary_accuracy: 0.7437\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 30s - loss: 0.4893 - binary_accuracy: 0.7512 - val_loss: 0.5320 - val_binary_accuracy: 0.7550\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 30s - loss: 0.4898 - binary_accuracy: 0.7548 - val_loss: 0.5826 - val_binary_accuracy: 0.7298\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 30s - loss: 0.4861 - binary_accuracy: 0.7572 - val_loss: 0.5293 - val_binary_accuracy: 0.7576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1874af93be0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_close_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modified version of Hilbert-CNN\n",
    "# batchnorm after activation\n",
    "# wide network\n",
    "# Max pooling\n",
    "\n",
    "def computation_block(in_layer, n_filters, filtersize_a, filtersize_b, filtersize_c, filtersize_d):\n",
    "\n",
    "    # residual 1\n",
    "    p1 = Conv2D(n_filters, (filtersize_a, filtersize_a), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "    p1 = Conv2D(n_filters, (filtersize_b, filtersize_b), strides=(1, 1), padding='same')(p1)\n",
    "    #p1 = BatchNormalization()(p1)\n",
    "\n",
    "    # residual 2\n",
    "    p2 = Conv2D(n_filters, (filtersize_c, filtersize_c), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "    p2 = Conv2D(n_filters, (filtersize_d, filtersize_d), strides=(1, 1), padding='same')(p2)\n",
    "    #p2 = BatchNormalization()(p2)\n",
    "\n",
    "    x = Concatenate()([in_layer, p1, p2])\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "# stem\n",
    "inputs = Input(shape=[32, 32, 4])\n",
    "x = GaussianNoise(0.3)(inputs)\n",
    "x = Conv2D(256, (7, 7), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (5, 5), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = computation_block(x, 32, 8, 4, 4, 3)\n",
    "x = computation_block(x, 32, 3, 3, 3, 3)\n",
    "# mid-stem\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = computation_block(x, 32, 2, 4, 4, 3)\n",
    "x = computation_block(x, 32, 2, 2, 2, 2)\n",
    "x = computation_block(x, 32, 3, 2, 2, 3)\n",
    "\n",
    "\n",
    "# exit stem\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# FC layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer= SGD(lr= 1e-2, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65489, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_wide_hilbertcnn/weights-01-0.65.hdf5\n",
      " - 126s - loss: 0.6923 - binary_accuracy: 0.6381 - val_loss: 0.6549 - val_binary_accuracy: 0.6859\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65489 to 0.56805, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_wide_hilbertcnn/weights-02-0.57.hdf5\n",
      " - 117s - loss: 0.5759 - binary_accuracy: 0.6970 - val_loss: 0.5681 - val_binary_accuracy: 0.7256\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 116s - loss: 0.5597 - binary_accuracy: 0.6994 - val_loss: 0.5941 - val_binary_accuracy: 0.6983\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56805 to 0.55098, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_wide_hilbertcnn/weights-04-0.55.hdf5\n",
      " - 117s - loss: 0.5463 - binary_accuracy: 0.7127 - val_loss: 0.5510 - val_binary_accuracy: 0.7303\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 117s - loss: 0.5413 - binary_accuracy: 0.7198 - val_loss: 0.6013 - val_binary_accuracy: 0.7127\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 118s - loss: 0.5317 - binary_accuracy: 0.7217 - val_loss: 0.7148 - val_binary_accuracy: 0.7107\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 116s - loss: 0.5309 - binary_accuracy: 0.7275 - val_loss: 0.6397 - val_binary_accuracy: 0.7251\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 116s - loss: 0.5221 - binary_accuracy: 0.7350 - val_loss: 0.5863 - val_binary_accuracy: 0.7447\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 116s - loss: 0.5153 - binary_accuracy: 0.7371 - val_loss: 0.6040 - val_binary_accuracy: 0.7282\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 119s - loss: 0.5154 - binary_accuracy: 0.7390 - val_loss: 0.6136 - val_binary_accuracy: 0.7277\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 118s - loss: 0.5079 - binary_accuracy: 0.7444 - val_loss: 0.8586 - val_binary_accuracy: 0.6921\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 124s - loss: 0.5059 - binary_accuracy: 0.7433 - val_loss: 1.2436 - val_binary_accuracy: 0.6343\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 116s - loss: 0.5139 - binary_accuracy: 0.7383 - val_loss: 0.8653 - val_binary_accuracy: 0.7143\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 116s - loss: 0.5006 - binary_accuracy: 0.7474 - val_loss: 0.8775 - val_binary_accuracy: 0.7205\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-bcf218809661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                     callbacks = [save_model, csv_logger, lr_scheduler])\n\u001b[0m",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2177\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1849\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_wide_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 32, 32, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_7 (GaussianNoise (None, 32, 32, 4)    0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 32, 32, 256)  50432       gaussian_noise_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 32, 32, 256)  1024        conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 32, 32, 256)  1638656     batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 32, 32, 256)  1024        conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 16, 16, 256)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 16, 16, 32)   524320      average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 16, 16, 32)   131104      average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 16, 16, 32)   128         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 16, 16, 32)   128         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 16, 16, 32)   16416       batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 16, 16, 32)   9248        batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 16, 16, 320)  0           average_pooling2d_10[0][0]       \n",
      "                                                                 conv2d_136[0][0]                 \n",
      "                                                                 conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 320)  0           concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 16, 16, 320)  1280        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 16, 16, 32)   92192       batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 16, 16, 32)   92192       batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 16, 16, 32)   128         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 16, 16, 32)   128         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 16, 16, 32)   9248        batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 16, 16, 32)   9248        batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 16, 16, 384)  0           batch_normalization_133[0][0]    \n",
      "                                                                 conv2d_140[0][0]                 \n",
      "                                                                 conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 384)  0           concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 16, 16, 384)  1536        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 8, 8, 384)    0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 8, 8, 32)     49184       average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 8, 8, 32)     196640      average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 8, 8, 32)     128         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 8, 8, 32)     128         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 8, 8, 32)     16416       batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 8, 8, 32)     9248        batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 448)    0           average_pooling2d_11[0][0]       \n",
      "                                                                 conv2d_144[0][0]                 \n",
      "                                                                 conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 448)    0           concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 8, 8, 448)    1792        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 8, 8, 32)     57376       batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 8, 8, 32)     57376       batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 8, 8, 32)     128         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 8, 8, 32)     128         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 8, 8, 32)     4128        batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 8, 8, 32)     4128        batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 512)    0           batch_normalization_139[0][0]    \n",
      "                                                                 conv2d_148[0][0]                 \n",
      "                                                                 conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 512)    0           concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 8, 8, 512)    2048        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 8, 8, 32)     147488      batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 8, 8, 32)     65568       batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 8, 8, 32)     128         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 8, 8, 32)     128         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 8, 8, 32)     4128        batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 8, 8, 32)     9248        batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 576)    0           batch_normalization_142[0][0]    \n",
      "                                                                 conv2d_152[0][0]                 \n",
      "                                                                 conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 576)    0           concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 8, 8, 576)    2304        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 4, 4, 576)    0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 9216)         0           average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1024)         9438208     flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1024)         0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1)            1025        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,645,505\n",
      "Trainable params: 12,639,361\n",
      "Non-trainable params: 6,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# original implementation of Hilbert-CNN\n",
    "# https://openreview.net/forum?id=HJvvRoe0W\n",
    "# no dimension reduction before dense \n",
    "\n",
    "def computation_block(in_layer, n_filters, filtersize_a, filtersize_b, filtersize_c, filtersize_d):\n",
    "\n",
    "    # residual 1\n",
    "    p1 = Conv2D(n_filters, (filtersize_a, filtersize_a), strides=(1, 1), padding='same')(in_layer)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "    p1 = Activation('relu')(p1)\n",
    "    p1 = Conv2D(n_filters, (filtersize_b, filtersize_b), strides=(1, 1), padding='same')(p1)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "\n",
    "    # residual 2\n",
    "    p2 = Conv2D(n_filters, (filtersize_c, filtersize_c), strides=(1, 1), padding='same')(in_layer)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "    p2 = Activation('relu')(p2)\n",
    "    p2 = Conv2D(n_filters, (filtersize_d, filtersize_d), strides=(1, 1), padding='same')(p2)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "\n",
    "    x = Concatenate()([in_layer, p1, p2])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# stem\n",
    "inputs = Input(shape=[32, 32, 4])\n",
    "x = GaussianNoise(0.2)(inputs)\n",
    "x = Conv2D(64, (7, 7), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (5, 5), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = computation_block(x, 4, 8, 4, 4, 3)\n",
    "x = computation_block(x, 4, 3, 3, 3, 3)\n",
    "# mid-stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = computation_block(x, 4, 2, 4, 4, 3)\n",
    "x = computation_block(x, 4, 2, 2, 2, 2)\n",
    "x = computation_block(x, 4, 3, 2, 2, 3)\n",
    "\n",
    "\n",
    "# exit stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "#x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x) #we omit this last avgpool to retain dimensionality\n",
    "x = Flatten()(x)\n",
    "\n",
    "# FC layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer= SGD(lr= 0.01, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55333, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/highdimdropout_hilbertcnn/weights-01-0.55.hdf5\n",
      " - 53s - loss: 0.5959 - binary_accuracy: 0.6777 - val_loss: 0.5533 - val_binary_accuracy: 0.7158\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 36s - loss: 0.5549 - binary_accuracy: 0.7085 - val_loss: 0.6000 - val_binary_accuracy: 0.6787\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55333 to 0.52281, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/highdimdropout_hilbertcnn/weights-03-0.52.hdf5\n",
      " - 36s - loss: 0.5368 - binary_accuracy: 0.7241 - val_loss: 0.5228 - val_binary_accuracy: 0.7463\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 36s - loss: 0.5326 - binary_accuracy: 0.7259 - val_loss: 0.5369 - val_binary_accuracy: 0.7179\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.52281 to 0.51251, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/highdimdropout_hilbertcnn/weights-05-0.51.hdf5\n",
      " - 37s - loss: 0.5185 - binary_accuracy: 0.7345 - val_loss: 0.5125 - val_binary_accuracy: 0.7189\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 36s - loss: 0.5142 - binary_accuracy: 0.7406 - val_loss: 0.5143 - val_binary_accuracy: 0.7292\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 36s - loss: 0.5097 - binary_accuracy: 0.7359 - val_loss: 0.5167 - val_binary_accuracy: 0.7354\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 36s - loss: 0.5014 - binary_accuracy: 0.7436 - val_loss: 0.5344 - val_binary_accuracy: 0.7298\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51251 to 0.49097, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/highdimdropout_hilbertcnn/weights-09-0.49.hdf5\n",
      " - 36s - loss: 0.5040 - binary_accuracy: 0.7457 - val_loss: 0.4910 - val_binary_accuracy: 0.7514\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 36s - loss: 0.4963 - binary_accuracy: 0.7469 - val_loss: 0.5069 - val_binary_accuracy: 0.7421\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 36s - loss: 0.4979 - binary_accuracy: 0.7467 - val_loss: 0.5048 - val_binary_accuracy: 0.7406\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 36s - loss: 0.4918 - binary_accuracy: 0.7507 - val_loss: 0.5139 - val_binary_accuracy: 0.7442\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 37s - loss: 0.4889 - binary_accuracy: 0.7524 - val_loss: 0.5113 - val_binary_accuracy: 0.7473\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 36s - loss: 0.4895 - binary_accuracy: 0.7510 - val_loss: 0.5050 - val_binary_accuracy: 0.7411\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 36s - loss: 0.4889 - binary_accuracy: 0.7535 - val_loss: 0.5324 - val_binary_accuracy: 0.7071\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 36s - loss: 0.4826 - binary_accuracy: 0.7582 - val_loss: 0.5048 - val_binary_accuracy: 0.7334\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 36s - loss: 0.4891 - binary_accuracy: 0.7532 - val_loss: 0.5066 - val_binary_accuracy: 0.7267\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 36s - loss: 0.4789 - binary_accuracy: 0.7627 - val_loss: 0.5098 - val_binary_accuracy: 0.7385\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 36s - loss: 0.4800 - binary_accuracy: 0.7582 - val_loss: 0.5067 - val_binary_accuracy: 0.7401\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 36s - loss: 0.4809 - binary_accuracy: 0.7588 - val_loss: 0.5156 - val_binary_accuracy: 0.7231\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 36s - loss: 0.4795 - binary_accuracy: 0.7572 - val_loss: 0.5304 - val_binary_accuracy: 0.7184\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 36s - loss: 0.4703 - binary_accuracy: 0.7621 - val_loss: 0.5275 - val_binary_accuracy: 0.7184\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-7c5c4d220891>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                     callbacks = [save_model, csv_logger, lr_scheduler])\n\u001b[0m",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2177\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1849\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/highdimdropout_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# original implementation of Hilbert-CNN\n",
    "# https://openreview.net/forum?id=HJvvRoe0W\n",
    "# has averagepooling2D before dense\n",
    "\n",
    "def computation_block(in_layer, n_filters, filtersize_a, filtersize_b, filtersize_c, filtersize_d):\n",
    "\n",
    "    # residual 1\n",
    "    p1 = Conv2D(n_filters, (filtersize_a, filtersize_a), strides=(1, 1), padding='same')(in_layer)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "    p1 = Activation('relu')(p1)\n",
    "    p1 = Conv2D(n_filters, (filtersize_b, filtersize_b), strides=(1, 1), padding='same')(p1)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "\n",
    "    # residual 2\n",
    "    p2 = Conv2D(n_filters, (filtersize_c, filtersize_c), strides=(1, 1), padding='same')(in_layer)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "    p2 = Activation('relu')(p2)\n",
    "    p2 = Conv2D(n_filters, (filtersize_d, filtersize_d), strides=(1, 1), padding='same')(p2)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "\n",
    "    x = Concatenate()([in_layer, p1, p2])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# stem\n",
    "inputs = Input(shape=[32, 32, 4])\n",
    "x = GaussianNoise(0.2)(inputs)\n",
    "x = Conv2D(64, (7, 7), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (5, 5), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = computation_block(x, 4, 8, 4, 4, 3)\n",
    "x = computation_block(x, 4, 3, 3, 3, 3)\n",
    "# mid-stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = computation_block(x, 4, 2, 4, 4, 3)\n",
    "x = computation_block(x, 4, 2, 2, 2, 2)\n",
    "x = computation_block(x, 4, 3, 2, 2, 3)\n",
    "\n",
    "\n",
    "# exit stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x) \n",
    "x = Flatten()(x)\n",
    "\n",
    "# FC layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer= SGD(lr= 0.01, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54871, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/lowdimdropout_hilbertcnn/weights-01-0.55.hdf5\n",
      " - 55s - loss: 0.5959 - binary_accuracy: 0.6698 - val_loss: 0.5487 - val_binary_accuracy: 0.7133\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 36s - loss: 0.5505 - binary_accuracy: 0.7138 - val_loss: 0.5928 - val_binary_accuracy: 0.6699\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 36s - loss: 0.5381 - binary_accuracy: 0.7192 - val_loss: 0.5581 - val_binary_accuracy: 0.6978\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54871 to 0.53085, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/lowdimdropout_hilbertcnn/weights-04-0.53.hdf5\n",
      " - 36s - loss: 0.5316 - binary_accuracy: 0.7247 - val_loss: 0.5309 - val_binary_accuracy: 0.7153\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 36s - loss: 0.5239 - binary_accuracy: 0.7333 - val_loss: 0.5351 - val_binary_accuracy: 0.7236\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 36s - loss: 0.5169 - binary_accuracy: 0.7345 - val_loss: 0.5313 - val_binary_accuracy: 0.7313\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.53085 to 0.49480, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/lowdimdropout_hilbertcnn/weights-07-0.49.hdf5\n",
      " - 36s - loss: 0.5100 - binary_accuracy: 0.7427 - val_loss: 0.4948 - val_binary_accuracy: 0.7463\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 36s - loss: 0.5071 - binary_accuracy: 0.7446 - val_loss: 0.5142 - val_binary_accuracy: 0.7256\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 36s - loss: 0.5042 - binary_accuracy: 0.7451 - val_loss: 0.5175 - val_binary_accuracy: 0.7287\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 35s - loss: 0.5035 - binary_accuracy: 0.7496 - val_loss: 0.4980 - val_binary_accuracy: 0.7375\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 36s - loss: 0.4971 - binary_accuracy: 0.7484 - val_loss: 0.5025 - val_binary_accuracy: 0.7447\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 36s - loss: 0.4993 - binary_accuracy: 0.7494 - val_loss: 0.5188 - val_binary_accuracy: 0.7189\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 36s - loss: 0.4968 - binary_accuracy: 0.7461 - val_loss: 0.4975 - val_binary_accuracy: 0.7406\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 36s - loss: 0.4918 - binary_accuracy: 0.7529 - val_loss: 0.5031 - val_binary_accuracy: 0.7483\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 36s - loss: 0.4919 - binary_accuracy: 0.7530 - val_loss: 0.5126 - val_binary_accuracy: 0.7298\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 36s - loss: 0.4881 - binary_accuracy: 0.7551 - val_loss: 0.5069 - val_binary_accuracy: 0.7323\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 36s - loss: 0.4880 - binary_accuracy: 0.7571 - val_loss: 0.5034 - val_binary_accuracy: 0.7468\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 35s - loss: 0.4808 - binary_accuracy: 0.7586 - val_loss: 0.5277 - val_binary_accuracy: 0.7127\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.49480 to 0.49042, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/lowdimdropout_hilbertcnn/weights-19-0.49.hdf5\n",
      " - 36s - loss: 0.4852 - binary_accuracy: 0.7571 - val_loss: 0.4904 - val_binary_accuracy: 0.7519\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 36s - loss: 0.4839 - binary_accuracy: 0.7563 - val_loss: 0.5160 - val_binary_accuracy: 0.7261\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 36s - loss: 0.4796 - binary_accuracy: 0.7576 - val_loss: 0.5165 - val_binary_accuracy: 0.7231\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 35s - loss: 0.4824 - binary_accuracy: 0.7604 - val_loss: 0.5012 - val_binary_accuracy: 0.7447\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 36s - loss: 0.4738 - binary_accuracy: 0.7626 - val_loss: 0.5112 - val_binary_accuracy: 0.7390\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 36s - loss: 0.4782 - binary_accuracy: 0.7617 - val_loss: 0.4955 - val_binary_accuracy: 0.7354\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 36s - loss: 0.4771 - binary_accuracy: 0.7594 - val_loss: 0.5049 - val_binary_accuracy: 0.7344\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 36s - loss: 0.4744 - binary_accuracy: 0.7651 - val_loss: 0.5135 - val_binary_accuracy: 0.7256\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 36s - loss: 0.4737 - binary_accuracy: 0.7652 - val_loss: 0.5072 - val_binary_accuracy: 0.7308\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 36s - loss: 0.4688 - binary_accuracy: 0.7652 - val_loss: 0.5058 - val_binary_accuracy: 0.7339\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 36s - loss: 0.4753 - binary_accuracy: 0.7644 - val_loss: 0.5086 - val_binary_accuracy: 0.7380\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 36s - loss: 0.4701 - binary_accuracy: 0.7652 - val_loss: 0.5148 - val_binary_accuracy: 0.7277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1878a000748>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/lowdimdropout_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications import mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\applications\\imagenet_utils.py:257: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 4 input channels.\n",
      "  str(input_shape[-1]) + ' input channels.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 32, 32, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 16, 16, 32)        1152      \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 16, 16, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 16, 16, 64)        2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 8, 8, 64)          576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 8, 8, 128)         8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              8389632   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 8,404,193\n",
      "Trainable params: 8,403,553\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# truncated mobilenet\n",
    "\n",
    "head_model = MobileNet(include_top=False,\n",
    "                       weights=None,\n",
    "                       input_shape = (32, 32, 4))\n",
    "\n",
    "x = head_model.get_layer('conv_pw_2_relu').output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.8)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=head_model.input, outputs=predictions)\n",
    "model.compile(optimizer= SGD(lr= 0.01, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55217, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/mobile_hilbertcnn/weights-01-0.55.hdf5\n",
      " - 25s - loss: 0.6268 - binary_accuracy: 0.6661 - val_loss: 0.5522 - val_binary_accuracy: 0.7009\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55217 to 0.51647, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/mobile_hilbertcnn/weights-02-0.52.hdf5\n",
      " - 21s - loss: 0.5575 - binary_accuracy: 0.7048 - val_loss: 0.5165 - val_binary_accuracy: 0.7127\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 18s - loss: 0.5413 - binary_accuracy: 0.7157 - val_loss: 0.5225 - val_binary_accuracy: 0.7127\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.51647 to 0.50570, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/mobile_hilbertcnn/weights-04-0.51.hdf5\n",
      " - 19s - loss: 0.5272 - binary_accuracy: 0.7214 - val_loss: 0.5057 - val_binary_accuracy: 0.7303\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 18s - loss: 0.5269 - binary_accuracy: 0.7283 - val_loss: 0.5064 - val_binary_accuracy: 0.7390\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 18s - loss: 0.5150 - binary_accuracy: 0.7343 - val_loss: 0.5101 - val_binary_accuracy: 0.7323\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.50570 to 0.48813, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/mobile_hilbertcnn/weights-07-0.49.hdf5\n",
      " - 19s - loss: 0.5113 - binary_accuracy: 0.7366 - val_loss: 0.4881 - val_binary_accuracy: 0.7519\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 18s - loss: 0.5100 - binary_accuracy: 0.7332 - val_loss: 0.5002 - val_binary_accuracy: 0.7344\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 18s - loss: 0.5021 - binary_accuracy: 0.7405 - val_loss: 0.5186 - val_binary_accuracy: 0.7303\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.48813 to 0.48606, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/mobile_hilbertcnn/weights-10-0.49.hdf5\n",
      " - 19s - loss: 0.5015 - binary_accuracy: 0.7452 - val_loss: 0.4861 - val_binary_accuracy: 0.7519\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 18s - loss: 0.5027 - binary_accuracy: 0.7433 - val_loss: 0.4933 - val_binary_accuracy: 0.7344\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 18s - loss: 0.4958 - binary_accuracy: 0.7476 - val_loss: 0.5022 - val_binary_accuracy: 0.7339\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.48606 to 0.48146, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/mobile_hilbertcnn/weights-13-0.48.hdf5\n",
      " - 19s - loss: 0.4919 - binary_accuracy: 0.7502 - val_loss: 0.4815 - val_binary_accuracy: 0.7602\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 18s - loss: 0.4929 - binary_accuracy: 0.7507 - val_loss: 0.5108 - val_binary_accuracy: 0.7339\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 18s - loss: 0.4890 - binary_accuracy: 0.7548 - val_loss: 0.5008 - val_binary_accuracy: 0.7385\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 18s - loss: 0.4864 - binary_accuracy: 0.7513 - val_loss: 0.5003 - val_binary_accuracy: 0.7380\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 18s - loss: 0.4806 - binary_accuracy: 0.7590 - val_loss: 0.5003 - val_binary_accuracy: 0.7390\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 18s - loss: 0.4847 - binary_accuracy: 0.7546 - val_loss: 0.4888 - val_binary_accuracy: 0.7406\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 18s - loss: 0.4814 - binary_accuracy: 0.7602 - val_loss: 0.4980 - val_binary_accuracy: 0.7442\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 18s - loss: 0.4779 - binary_accuracy: 0.7639 - val_loss: 0.4978 - val_binary_accuracy: 0.7509\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 18s - loss: 0.4823 - binary_accuracy: 0.7582 - val_loss: 0.4897 - val_binary_accuracy: 0.7437\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 18s - loss: 0.4764 - binary_accuracy: 0.7642 - val_loss: 0.4934 - val_binary_accuracy: 0.7396\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 18s - loss: 0.4738 - binary_accuracy: 0.7625 - val_loss: 0.4968 - val_binary_accuracy: 0.7432\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 18s - loss: 0.4737 - binary_accuracy: 0.7607 - val_loss: 0.4991 - val_binary_accuracy: 0.7468\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 18s - loss: 0.4663 - binary_accuracy: 0.7698 - val_loss: 0.4944 - val_binary_accuracy: 0.7468\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 18s - loss: 0.4777 - binary_accuracy: 0.7617 - val_loss: 0.4912 - val_binary_accuracy: 0.7365\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 18s - loss: 0.4718 - binary_accuracy: 0.7631 - val_loss: 0.5008 - val_binary_accuracy: 0.7437\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 18s - loss: 0.4663 - binary_accuracy: 0.7675 - val_loss: 0.4924 - val_binary_accuracy: 0.7452\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 18s - loss: 0.4680 - binary_accuracy: 0.7692 - val_loss: 0.5137 - val_binary_accuracy: 0.7349\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 18s - loss: 0.4663 - binary_accuracy: 0.7669 - val_loss: 0.4873 - val_binary_accuracy: 0.7442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x212b743aeb8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/mobile_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import os\n",
    "import keras.applications.mobilenet\n",
    "from keras.utils.generic_utils import CustomObjectScope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_path = 'D:/Projects/iSynPro/iSynPro/HilbertCNN/train_val_npys/6count/1mer/test/high'\n",
    "low_path = 'D:/Projects/iSynPro/iSynPro/HilbertCNN/train_val_npys/6count/1mer/test/low'\n",
    "\n",
    "high_xcomp = []\n",
    "high_ycomp = []\n",
    "for root, subdir, files in os.walk(high_path):\n",
    "    for file in files:\n",
    "        high_xcomp.append(np.load(os.path.join(root, file)))\n",
    "        high_ycomp.append(0)\n",
    "\n",
    "low_xcomp = []\n",
    "low_ycomp = []\n",
    "for root, subdir, files in os.walk(low_path):\n",
    "    for file in files:\n",
    "        low_xcomp.append(np.load(os.path.join(root, file)))\n",
    "        low_ycomp.append(1)\n",
    "    \n",
    "x_test = np.asarray(low_xcomp + high_xcomp)\n",
    "y_test = np.asarray(low_ycomp + high_ycomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['D:/Projects/iSynPro/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-18-0.50.hdf5',\n",
    "              'D:/Projects/iSynPro/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-16-0.50.hdf5',\n",
    "              'D:/Projects/iSynPro/SyntheticPromoter/HilbertCNN/weights/1mer/modified_close_hilbertcnn/weights-07-0.50.hdf5',\n",
    "              'D:/Projects/iSynPro/SyntheticPromoter/HilbertCNN/weights/1mer/modified_wide_hilbertcnn/weights-04-0.55.hdf5',\n",
    "              'D:/Projects/iSynPro/SyntheticPromoter/HilbertCNN/weights/1mer/highdimdropout_hilbertcnn/weights-09-0.49.hdf5',\n",
    "              'D:/Projects/iSynPro/SyntheticPromoter/HilbertCNN/weights/1mer/lowdimdropout_hilbertcnn/weights-19-0.49.hdf5',\n",
    "              'D:/Projects/iSynPro/SyntheticPromoter/HilbertCNN/weights/1mer/mobile_hilbertcnn/weights-13-0.48.hdf5'\n",
    "             ]\n",
    "label_list = ['Vanilla Hilbert-CNN',\n",
    "              'Batchnorm -BlockRelu +FinalRelu',\n",
    "              'Batchnorm +BlockRelu -FinalRelu',\n",
    "              'Wide Hilbert-CNN',\n",
    "              'Hilbert-CNN -FinalMaxPool +Dropout',\n",
    "              'Hilbert-CNN +FinalMaxPool +Dropout',\n",
    "              'MobileNet truncated']\n",
    "roc_list = []\n",
    "for path in model_list:\n",
    "    with CustomObjectScope({'relu6': keras.applications.mobilenet.relu6,'DepthwiseConv2D': keras.applications.mobilenet.DepthwiseConv2D}):\n",
    "        model = load_model(path)\n",
    "        y_pred = model.predict(x_test)\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "        roc_list.append([fpr, tpr, auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwikEQRAEJcIpBEEQBCXCKQRBEAQl/gdrvQCVPS8aygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2281e7f9668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "palette = sns.color_palette(\"cubehelix\", len(roc_list))\n",
    "\n",
    "#plot roc curve\n",
    "for i in range(len(roc_list)):\n",
    "    plt.plot(roc_list[i][0], \n",
    "             roc_list[i][1], \n",
    "             color=palette[i], \n",
    "             label='{0} (AUC = {1:.3f})'.format(label_list[i], roc_list[i][2]))\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Hilbert CNNs')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('d:/projects/isynpro/SyntheticPromoter/readme_figures/hilbert_roc.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

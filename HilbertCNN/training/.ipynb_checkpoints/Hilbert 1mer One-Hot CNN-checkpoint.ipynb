{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adaptation of the Hilbert CNN at https://openreview.net/forum?id=HJvvRoe0W\n",
    "Works by encoding each nucleotide as a one-hot vector, \n",
    "then fits it to a image-like grid using a hilbert curve\n",
    "such that each 'pixel' is a 1mer of length 4\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, BatchNormalization, AveragePooling2D, Dense, Dropout, SeparableConv2D, Add\n",
    "from keras.layers import Activation, Input, Concatenate, Flatten, MaxPooling2D, Reshape, GaussianNoise\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "import image\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17588 images belonging to 2 classes.\n",
      "Found 1955 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "start_target_size = (32, 32, 4)\n",
    "batch_size = 16\n",
    "train_path = 'D:/Projects/iSynPro/iSynPro/HilbertCNN/train_val_npys/6count/1mer/train'\n",
    "test_path = 'D:/Projects/iSynPro/iSynPro/HilbertCNN/train_val_npys/6count/1mer/test'\n",
    "\n",
    "# define generators\n",
    "train_datagen = image.ImageDataGenerator()\n",
    "test_datagen = image.ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_np_from_directory(train_path, \n",
    "                                                    target_size= start_target_size, \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    class_mode='binary',\n",
    "                                                    seed=42) \n",
    "\n",
    "validation_generator = test_datagen.flow_np_from_directory(test_path, \n",
    "                                                        target_size= start_target_size, \n",
    "                                                        batch_size=batch_size, \n",
    "                                                        class_mode='binary',\n",
    "                                                        seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original implementation of Hilbert-CNN\n",
    "# https://openreview.net/forum?id=HJvvRoe0W\n",
    "\n",
    "def computation_block(in_layer, n_filters, filtersize_a, filtersize_b, filtersize_c, filtersize_d):\n",
    "\n",
    "    # residual 1\n",
    "    p1 = Conv2D(n_filters, (filtersize_a, filtersize_a), strides=(1, 1), padding='same')(in_layer)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "    p1 = Activation('relu')(p1)\n",
    "    p1 = Conv2D(n_filters, (filtersize_b, filtersize_b), strides=(1, 1), padding='same')(p1)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "\n",
    "    # residual 2\n",
    "    p2 = Conv2D(n_filters, (filtersize_c, filtersize_c), strides=(1, 1), padding='same')(in_layer)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "    p2 = Activation('relu')(p2)\n",
    "    p2 = Conv2D(n_filters, (filtersize_d, filtersize_d), strides=(1, 1), padding='same')(p2)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "\n",
    "    x = Concatenate()([in_layer, p1, p2])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# stem\n",
    "inputs = Input(shape=[32, 32, 4])\n",
    "x = GaussianNoise(0.2)(inputs)\n",
    "x = Conv2D(64, (7, 7), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (5, 5), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = computation_block(x, 4, 8, 4, 4, 3)\n",
    "x = computation_block(x, 4, 3, 3, 3, 3)\n",
    "# mid-stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = computation_block(x, 4, 2, 4, 4, 3)\n",
    "x = computation_block(x, 4, 2, 2, 2, 2)\n",
    "x = computation_block(x, 4, 3, 2, 2, 3)\n",
    "\n",
    "\n",
    "# exit stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x) #we omit this last avgpool to retain dimensionality\n",
    "x = Flatten()(x)\n",
    "\n",
    "# FC layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer= SGD(lr= 0.01, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 32, 32, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_9 (GaussianNoise (None, 32, 32, 4)    0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 32, 32, 64)   12608       gaussian_noise_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 32, 32, 64)   256         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 32, 32, 64)   102464      batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 32, 32, 64)   256         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 64)   0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, 16, 16, 64)   0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 16, 16, 4)    16388       average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 16, 16, 4)    4100        average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 16, 16, 4)    16          conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 16, 16, 4)    16          conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 4)    0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 4)    0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 16, 16, 4)    260         activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 16, 16, 4)    148         activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 16, 16, 4)    16          conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 16, 16, 4)    16          conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 16, 16, 72)   0           average_pooling2d_13[0][0]       \n",
      "                                                                 batch_normalization_166[0][0]    \n",
      "                                                                 batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 16, 16, 72)   288         concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 72)   0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 16, 16, 4)    2596        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 16, 16, 4)    2596        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 16, 16, 4)    16          conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 16, 16, 4)    16          conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 4)    0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 16, 16, 4)    0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 16, 16, 4)    148         activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 16, 16, 4)    148         activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 16, 16, 4)    16          conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 16, 16, 4)    16          conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 16, 16, 80)   0           activation_30[0][0]              \n",
      "                                                                 batch_normalization_171[0][0]    \n",
      "                                                                 batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 16, 16, 80)   320         concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 80)   0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 8, 8, 80)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 8, 8, 4)      1284        average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 8, 8, 4)      5124        average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 8, 8, 4)      16          conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 8, 8, 4)      16          conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 4)      0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 4)      0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 8, 8, 4)      260         activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 8, 8, 4)      148         activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 8, 8, 4)      16          conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 8, 8, 4)      16          conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 8, 8, 88)     0           average_pooling2d_14[0][0]       \n",
      "                                                                 batch_normalization_176[0][0]    \n",
      "                                                                 batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 8, 8, 88)     352         concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 88)     0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 8, 8, 4)      1412        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 8, 8, 4)      1412        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 8, 8, 4)      16          conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 8, 8, 4)      16          conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 4)      0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 4)      0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 8, 8, 4)      68          activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 8, 8, 4)      68          activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 8, 8, 4)      16          conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 8, 8, 4)      16          conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 8, 8, 96)     0           activation_36[0][0]              \n",
      "                                                                 batch_normalization_181[0][0]    \n",
      "                                                                 batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 8, 8, 96)     384         concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 96)     0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 8, 8, 4)      3460        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 8, 8, 4)      1540        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 8, 8, 4)      16          conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 8, 8, 4)      16          conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 4)      0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 8, 8, 4)      0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 8, 8, 4)      68          activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 8, 8, 4)      148         activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 8, 8, 4)      16          conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 8, 8, 4)      16          conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 8, 8, 104)    0           activation_39[0][0]              \n",
      "                                                                 batch_normalization_186[0][0]    \n",
      "                                                                 batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 8, 8, 104)    416         concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 8, 8, 104)    0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_15 (AveragePo (None, 4, 4, 104)    0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 4, 4, 104)    416         average_pooling2d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 104)    0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, 2, 2, 104)    0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 416)          0           average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1024)         427008      flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1024)         1049600     dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1)            1025        dense_18[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,637,089\n",
      "Trainable params: 1,635,585\n",
      "Non-trainable params: 1,504\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53822, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-01-0.54.hdf5\n",
      " - 48s - loss: 0.5807 - binary_accuracy: 0.6855 - val_loss: 0.5382 - val_binary_accuracy: 0.7339\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 35s - loss: 0.5468 - binary_accuracy: 0.7177 - val_loss: 0.5489 - val_binary_accuracy: 0.7091\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53822 to 0.53342, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-03-0.53.hdf5\n",
      " - 35s - loss: 0.5308 - binary_accuracy: 0.7234 - val_loss: 0.5334 - val_binary_accuracy: 0.7081\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.53342 to 0.53088, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-04-0.53.hdf5\n",
      " - 36s - loss: 0.5225 - binary_accuracy: 0.7317 - val_loss: 0.5309 - val_binary_accuracy: 0.7091\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.53088 to 0.52385, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-05-0.52.hdf5\n",
      " - 36s - loss: 0.5142 - binary_accuracy: 0.7370 - val_loss: 0.5238 - val_binary_accuracy: 0.7143\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.52385 to 0.50633, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-06-0.51.hdf5\n",
      " - 35s - loss: 0.5105 - binary_accuracy: 0.7355 - val_loss: 0.5063 - val_binary_accuracy: 0.7308\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 35s - loss: 0.5056 - binary_accuracy: 0.7457 - val_loss: 0.5150 - val_binary_accuracy: 0.7210\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 35s - loss: 0.4981 - binary_accuracy: 0.7504 - val_loss: 0.5087 - val_binary_accuracy: 0.7277\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50633 to 0.50555, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-09-0.51.hdf5\n",
      " - 36s - loss: 0.5002 - binary_accuracy: 0.7457 - val_loss: 0.5055 - val_binary_accuracy: 0.7313\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 35s - loss: 0.4960 - binary_accuracy: 0.7473 - val_loss: 0.5150 - val_binary_accuracy: 0.7287\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50555 to 0.50195, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-11-0.50.hdf5\n",
      " - 35s - loss: 0.4930 - binary_accuracy: 0.7522 - val_loss: 0.5019 - val_binary_accuracy: 0.7272\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.50195 to 0.49894, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-12-0.50.hdf5\n",
      " - 35s - loss: 0.4910 - binary_accuracy: 0.7522 - val_loss: 0.4989 - val_binary_accuracy: 0.7411\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 35s - loss: 0.4787 - binary_accuracy: 0.7607 - val_loss: 0.5066 - val_binary_accuracy: 0.7318\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 35s - loss: 0.4899 - binary_accuracy: 0.7524 - val_loss: 0.5015 - val_binary_accuracy: 0.7251\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 35s - loss: 0.4839 - binary_accuracy: 0.7575 - val_loss: 0.5099 - val_binary_accuracy: 0.7303\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.49894 to 0.49594, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-16-0.50.hdf5\n",
      " - 35s - loss: 0.4806 - binary_accuracy: 0.7593 - val_loss: 0.4959 - val_binary_accuracy: 0.7421\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 35s - loss: 0.4797 - binary_accuracy: 0.7604 - val_loss: 0.5256 - val_binary_accuracy: 0.7148\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.49594 to 0.49571, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-18-0.50.hdf5\n",
      " - 36s - loss: 0.4782 - binary_accuracy: 0.7590 - val_loss: 0.4957 - val_binary_accuracy: 0.7463\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 35s - loss: 0.4717 - binary_accuracy: 0.7644 - val_loss: 0.5092 - val_binary_accuracy: 0.7261\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 35s - loss: 0.4747 - binary_accuracy: 0.7671 - val_loss: 0.5115 - val_binary_accuracy: 0.7225\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 35s - loss: 0.4746 - binary_accuracy: 0.7644 - val_loss: 0.5205 - val_binary_accuracy: 0.7179\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 35s - loss: 0.4737 - binary_accuracy: 0.7630 - val_loss: 0.5138 - val_binary_accuracy: 0.7287\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 35s - loss: 0.4734 - binary_accuracy: 0.7643 - val_loss: 0.4975 - val_binary_accuracy: 0.7365\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 35s - loss: 0.4705 - binary_accuracy: 0.7634 - val_loss: 0.5306 - val_binary_accuracy: 0.7189\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 35s - loss: 0.4638 - binary_accuracy: 0.7705 - val_loss: 0.5142 - val_binary_accuracy: 0.7292\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 35s - loss: 0.4717 - binary_accuracy: 0.7668 - val_loss: 0.5184 - val_binary_accuracy: 0.7272\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 35s - loss: 0.4680 - binary_accuracy: 0.7665 - val_loss: 0.5061 - val_binary_accuracy: 0.7298\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 35s - loss: 0.4665 - binary_accuracy: 0.7662 - val_loss: 0.5168 - val_binary_accuracy: 0.7298\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 35s - loss: 0.4696 - binary_accuracy: 0.7666 - val_loss: 0.5116 - val_binary_accuracy: 0.7349\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 35s - loss: 0.4653 - binary_accuracy: 0.7687 - val_loss: 0.5044 - val_binary_accuracy: 0.7380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1876eb3d4e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "lr_descent = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                               factor=0.5,\n",
    "                                               patience=5,\n",
    "                                               verbose=1,\n",
    "                                               mode='auto',\n",
    "                                               epsilon=0.0001,\n",
    "                                               cooldown=1,\n",
    "                                               min_lr=0)\n",
    "\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "#tracking = keras.callbacks.ProgbarLogger(count_mode='samples')\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of Hilbert-CNN\n",
    "# batchnorm after activation\n",
    "\n",
    "def computation_block(in_layer, n_filters, filtersize_a, filtersize_b, filtersize_c, filtersize_d):\n",
    "\n",
    "    # residual 1\n",
    "    p1 = Conv2D(n_filters, (filtersize_a, filtersize_a), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "    p1 = Conv2D(n_filters, (filtersize_b, filtersize_b), strides=(1, 1), padding='same', activation='relu')(p1)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "\n",
    "    # residual 2\n",
    "    p2 = Conv2D(n_filters, (filtersize_c, filtersize_c), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "    p2 = Conv2D(n_filters, (filtersize_d, filtersize_d), strides=(1, 1), padding='same', activation='relu')(p2)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "\n",
    "    x = Concatenate()([in_layer, p1, p2])\n",
    "    #x = Activation('relu')(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "# stem\n",
    "inputs = Input(shape=[32, 32, 4])\n",
    "x = GaussianNoise(0.3)(inputs)\n",
    "x = Conv2D(64, (7, 7), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (5, 5), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = computation_block(x, 4, 8, 4, 4, 3)\n",
    "x = computation_block(x, 4, 3, 3, 3, 3)\n",
    "# mid-stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = computation_block(x, 4, 2, 4, 4, 3)\n",
    "x = computation_block(x, 4, 2, 2, 2, 2)\n",
    "x = computation_block(x, 4, 3, 2, 2, 3)\n",
    "\n",
    "\n",
    "# exit stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "#x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x) #we omit this last avgpool to retain dimensionality\n",
    "x = Flatten()(x)\n",
    "\n",
    "# FC layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer= SGD(lr= 1e-2, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73496, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-01-0.73.hdf5\n",
      " - 404s - loss: 0.6319 - binary_accuracy: 0.6322 - val_loss: 0.7350 - val_binary_accuracy: 0.5881\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73496 to 0.59861, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-02-0.60.hdf5\n",
      " - 32s - loss: 0.5682 - binary_accuracy: 0.6992 - val_loss: 0.5986 - val_binary_accuracy: 0.6674\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59861 to 0.58810, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-03-0.59.hdf5\n",
      " - 32s - loss: 0.5492 - binary_accuracy: 0.7133 - val_loss: 0.5881 - val_binary_accuracy: 0.7081\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58810 to 0.52329, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-04-0.52.hdf5\n",
      " - 32s - loss: 0.5391 - binary_accuracy: 0.7194 - val_loss: 0.5233 - val_binary_accuracy: 0.7380\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 32s - loss: 0.5277 - binary_accuracy: 0.7275 - val_loss: 0.5254 - val_binary_accuracy: 0.7256\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 32s - loss: 0.5262 - binary_accuracy: 0.7300 - val_loss: 0.5443 - val_binary_accuracy: 0.7215\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.52329 to 0.50540, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-07-0.51.hdf5\n",
      " - 32s - loss: 0.5191 - binary_accuracy: 0.7359 - val_loss: 0.5054 - val_binary_accuracy: 0.7447\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 32s - loss: 0.5165 - binary_accuracy: 0.7341 - val_loss: 0.6366 - val_binary_accuracy: 0.6735\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 32s - loss: 0.5112 - binary_accuracy: 0.7379 - val_loss: 0.5247 - val_binary_accuracy: 0.7282\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 32s - loss: 0.5124 - binary_accuracy: 0.7387 - val_loss: 0.5198 - val_binary_accuracy: 0.7432\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 32s - loss: 0.5064 - binary_accuracy: 0.7407 - val_loss: 0.5158 - val_binary_accuracy: 0.7447\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 32s - loss: 0.5034 - binary_accuracy: 0.7443 - val_loss: 0.5297 - val_binary_accuracy: 0.7287\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 32s - loss: 0.5050 - binary_accuracy: 0.7436 - val_loss: 0.5254 - val_binary_accuracy: 0.7339\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 32s - loss: 0.5002 - binary_accuracy: 0.7498 - val_loss: 0.5253 - val_binary_accuracy: 0.7396\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 32s - loss: 0.4999 - binary_accuracy: 0.7486 - val_loss: 0.5254 - val_binary_accuracy: 0.7308\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.50540 to 0.49668, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-16-0.50.hdf5\n",
      " - 32s - loss: 0.4976 - binary_accuracy: 0.7493 - val_loss: 0.4967 - val_binary_accuracy: 0.7416\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 32s - loss: 0.4960 - binary_accuracy: 0.7485 - val_loss: 0.5207 - val_binary_accuracy: 0.7272\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 32s - loss: 0.4965 - binary_accuracy: 0.7481 - val_loss: 0.5145 - val_binary_accuracy: 0.7282\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 32s - loss: 0.4950 - binary_accuracy: 0.7501 - val_loss: 0.5080 - val_binary_accuracy: 0.7344\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 32s - loss: 0.4946 - binary_accuracy: 0.7518 - val_loss: 0.5388 - val_binary_accuracy: 0.7272\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 32s - loss: 0.4902 - binary_accuracy: 0.7539 - val_loss: 0.5051 - val_binary_accuracy: 0.7339\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 32s - loss: 0.4930 - binary_accuracy: 0.7488 - val_loss: 0.5223 - val_binary_accuracy: 0.7272\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 32s - loss: 0.4903 - binary_accuracy: 0.7548 - val_loss: 0.5169 - val_binary_accuracy: 0.7339\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 32s - loss: 0.4874 - binary_accuracy: 0.7562 - val_loss: 0.5146 - val_binary_accuracy: 0.7365\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 32s - loss: 0.4899 - binary_accuracy: 0.7554 - val_loss: 0.5129 - val_binary_accuracy: 0.7437\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 32s - loss: 0.4887 - binary_accuracy: 0.7544 - val_loss: 0.5139 - val_binary_accuracy: 0.7359\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 32s - loss: 0.4846 - binary_accuracy: 0.7568 - val_loss: 0.5280 - val_binary_accuracy: 0.7236\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 32s - loss: 0.4866 - binary_accuracy: 0.7564 - val_loss: 0.4991 - val_binary_accuracy: 0.7396\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 31s - loss: 0.4843 - binary_accuracy: 0.7574 - val_loss: 0.5169 - val_binary_accuracy: 0.7267\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 32s - loss: 0.4843 - binary_accuracy: 0.7540 - val_loss: 0.5140 - val_binary_accuracy: 0.7359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18741cdd470>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNoise (None, 32, 32, 4)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 64)   12608       gaussian_noise_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 64)   102464      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 4)    16388       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 4)    4100        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 4)    16          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 4)    16          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 4)    260         batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 4)    148         batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 4)    16          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 4)    16          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 72)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 72)   0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 72)   288         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 4)    2596        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 4)    2596        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 4)    16          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 4)    16          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 4)    148         batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 4)    148         batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 4)    16          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 4)    16          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 80)   0           batch_normalization_7[0][0]      \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 80)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 80)   320         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 80)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 4)      1284        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 4)      5124        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 4)      16          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 4)      16          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 4)      260         batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 4)      148         batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 4)      16          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 4)      16          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8, 8, 88)     0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 8, 88)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 88)     352         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 4)      1412        batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 4)      1412        batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 4)      16          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 4)      16          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 4)      68          batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 4)      68          batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 4)      16          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 4)      16          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 8, 8, 96)     0           batch_normalization_17[0][0]     \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "                                                                 batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 8, 8, 96)     0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 96)     384         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 4)      3460        batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 4)      1540        batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 4)      16          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 4)      16          conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 4)      68          batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 4)      148         batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 4)      16          conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 4)      16          conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 8, 8, 104)    0           batch_normalization_22[0][0]     \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 8, 104)    0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 104)    416         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 104)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 4, 104)    416         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 4, 4, 104)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 2, 2, 104)    0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 416)          0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         427008      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1025        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 587,489\n",
      "Trainable params: 585,985\n",
      "Non-trainable params: 1,504\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of Hilbert-CNN\n",
    "# batchnorm after activation\n",
    "\n",
    "def computation_block(in_layer, n_filters, filtersize_a, filtersize_b, filtersize_c, filtersize_d):\n",
    "\n",
    "    # residual 1\n",
    "    p1 = Conv2D(n_filters, (filtersize_a, filtersize_a), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "    p1 = Conv2D(n_filters, (filtersize_b, filtersize_b), strides=(1, 1), padding='same')(p1)\n",
    "    #p1 = BatchNormalization()(p1)\n",
    "\n",
    "    # residual 2\n",
    "    p2 = Conv2D(n_filters, (filtersize_c, filtersize_c), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "    p2 = Conv2D(n_filters, (filtersize_d, filtersize_d), strides=(1, 1), padding='same')(p2)\n",
    "    #p2 = BatchNormalization()(p2)\n",
    "\n",
    "    x = Concatenate()([in_layer, p1, p2])\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "# stem\n",
    "inputs = Input(shape=[32, 32, 4])\n",
    "x = GaussianNoise(0.3)(inputs)\n",
    "x = Conv2D(64, (7, 7), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (5, 5), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = computation_block(x, 4, 8, 4, 4, 3)\n",
    "x = computation_block(x, 4, 3, 3, 3, 3)\n",
    "# mid-stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = computation_block(x, 4, 2, 4, 4, 3)\n",
    "x = computation_block(x, 4, 2, 2, 2, 2)\n",
    "x = computation_block(x, 4, 3, 2, 2, 3)\n",
    "\n",
    "\n",
    "# exit stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Activation('relu')(x)\n",
    "#x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x) #we omit this last avgpool to retain dimensionality\n",
    "x = Flatten()(x)\n",
    "\n",
    "# FC layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer= SGD(lr= 1e-2, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59563, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_close_hilbertcnn/weights-01-0.60.hdf5\n",
      " - 38s - loss: 0.6171 - binary_accuracy: 0.6597 - val_loss: 0.5956 - val_binary_accuracy: 0.6973\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59563 to 0.53943, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_close_hilbertcnn/weights-02-0.54.hdf5\n",
      " - 31s - loss: 0.5720 - binary_accuracy: 0.6952 - val_loss: 0.5394 - val_binary_accuracy: 0.7241\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 30s - loss: 0.5519 - binary_accuracy: 0.7106 - val_loss: 0.6027 - val_binary_accuracy: 0.6906\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.53943 to 0.51902, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_close_hilbertcnn/weights-04-0.52.hdf5\n",
      " - 31s - loss: 0.5408 - binary_accuracy: 0.7163 - val_loss: 0.5190 - val_binary_accuracy: 0.7329\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 30s - loss: 0.5301 - binary_accuracy: 0.7269 - val_loss: 0.6306 - val_binary_accuracy: 0.6663\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 30s - loss: 0.5269 - binary_accuracy: 0.7281 - val_loss: 0.5373 - val_binary_accuracy: 0.7127\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51902 to 0.50467, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_close_hilbertcnn/weights-07-0.50.hdf5\n",
      " - 31s - loss: 0.5210 - binary_accuracy: 0.7357 - val_loss: 0.5047 - val_binary_accuracy: 0.7561\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 30s - loss: 0.5190 - binary_accuracy: 0.7316 - val_loss: 0.5325 - val_binary_accuracy: 0.7478\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 31s - loss: 0.5149 - binary_accuracy: 0.7367 - val_loss: 0.5425 - val_binary_accuracy: 0.7447\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 30s - loss: 0.5090 - binary_accuracy: 0.7412 - val_loss: 0.5165 - val_binary_accuracy: 0.7437\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 30s - loss: 0.5084 - binary_accuracy: 0.7395 - val_loss: 0.5360 - val_binary_accuracy: 0.7344\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 31s - loss: 0.5062 - binary_accuracy: 0.7428 - val_loss: 0.5371 - val_binary_accuracy: 0.7380\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 31s - loss: 0.5045 - binary_accuracy: 0.7457 - val_loss: 0.5752 - val_binary_accuracy: 0.7096\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 31s - loss: 0.5011 - binary_accuracy: 0.7457 - val_loss: 0.5250 - val_binary_accuracy: 0.7380\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 31s - loss: 0.5003 - binary_accuracy: 0.7449 - val_loss: 0.5443 - val_binary_accuracy: 0.7421\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 30s - loss: 0.4997 - binary_accuracy: 0.7451 - val_loss: 0.5278 - val_binary_accuracy: 0.7401\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 30s - loss: 0.4997 - binary_accuracy: 0.7470 - val_loss: 0.5310 - val_binary_accuracy: 0.7329\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 30s - loss: 0.4987 - binary_accuracy: 0.7485 - val_loss: 0.5390 - val_binary_accuracy: 0.7473\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 30s - loss: 0.4984 - binary_accuracy: 0.7497 - val_loss: 0.5470 - val_binary_accuracy: 0.7390\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 30s - loss: 0.4944 - binary_accuracy: 0.7496 - val_loss: 0.5353 - val_binary_accuracy: 0.7473\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 30s - loss: 0.4939 - binary_accuracy: 0.7512 - val_loss: 0.5463 - val_binary_accuracy: 0.7365\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 31s - loss: 0.4922 - binary_accuracy: 0.7509 - val_loss: 0.5544 - val_binary_accuracy: 0.7354\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 30s - loss: 0.4900 - binary_accuracy: 0.7560 - val_loss: 0.5296 - val_binary_accuracy: 0.7488\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 31s - loss: 0.4923 - binary_accuracy: 0.7549 - val_loss: 0.5351 - val_binary_accuracy: 0.7318\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 30s - loss: 0.4909 - binary_accuracy: 0.7513 - val_loss: 0.5315 - val_binary_accuracy: 0.7540\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 31s - loss: 0.4904 - binary_accuracy: 0.7557 - val_loss: 0.5482 - val_binary_accuracy: 0.7287\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 30s - loss: 0.4892 - binary_accuracy: 0.7505 - val_loss: 0.5240 - val_binary_accuracy: 0.7437\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 30s - loss: 0.4893 - binary_accuracy: 0.7512 - val_loss: 0.5320 - val_binary_accuracy: 0.7550\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 30s - loss: 0.4898 - binary_accuracy: 0.7548 - val_loss: 0.5826 - val_binary_accuracy: 0.7298\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 30s - loss: 0.4861 - binary_accuracy: 0.7572 - val_loss: 0.5293 - val_binary_accuracy: 0.7576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1874af93be0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_close_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of Hilbert-CNN\n",
    "# batchnorm after activation\n",
    "# wide network\n",
    "# Max pooling\n",
    "\n",
    "def computation_block(in_layer, n_filters, filtersize_a, filtersize_b, filtersize_c, filtersize_d):\n",
    "\n",
    "    # residual 1\n",
    "    p1 = Conv2D(n_filters, (filtersize_a, filtersize_a), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "    p1 = Conv2D(n_filters, (filtersize_b, filtersize_b), strides=(1, 1), padding='same')(p1)\n",
    "    #p1 = BatchNormalization()(p1)\n",
    "\n",
    "    # residual 2\n",
    "    p2 = Conv2D(n_filters, (filtersize_c, filtersize_c), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "    p2 = Conv2D(n_filters, (filtersize_d, filtersize_d), strides=(1, 1), padding='same')(p2)\n",
    "    #p2 = BatchNormalization()(p2)\n",
    "\n",
    "    x = Concatenate()([in_layer, p1, p2])\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "# stem\n",
    "inputs = Input(shape=[32, 32, 4])\n",
    "x = GaussianNoise(0.3)(inputs)\n",
    "x = Conv2D(256, (7, 7), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (5, 5), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = computation_block(x, 32, 8, 4, 4, 3)\n",
    "x = computation_block(x, 32, 3, 3, 3, 3)\n",
    "# mid-stem\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = computation_block(x, 32, 2, 4, 4, 3)\n",
    "x = computation_block(x, 32, 2, 2, 2, 2)\n",
    "x = computation_block(x, 32, 3, 2, 2, 3)\n",
    "\n",
    "\n",
    "# exit stem\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# FC layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer= SGD(lr= 1e-2, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65489, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_wide_hilbertcnn/weights-01-0.65.hdf5\n",
      " - 126s - loss: 0.6923 - binary_accuracy: 0.6381 - val_loss: 0.6549 - val_binary_accuracy: 0.6859\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65489 to 0.56805, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_wide_hilbertcnn/weights-02-0.57.hdf5\n",
      " - 117s - loss: 0.5759 - binary_accuracy: 0.6970 - val_loss: 0.5681 - val_binary_accuracy: 0.7256\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 116s - loss: 0.5597 - binary_accuracy: 0.6994 - val_loss: 0.5941 - val_binary_accuracy: 0.6983\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56805 to 0.55098, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_wide_hilbertcnn/weights-04-0.55.hdf5\n",
      " - 117s - loss: 0.5463 - binary_accuracy: 0.7127 - val_loss: 0.5510 - val_binary_accuracy: 0.7303\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 117s - loss: 0.5413 - binary_accuracy: 0.7198 - val_loss: 0.6013 - val_binary_accuracy: 0.7127\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 118s - loss: 0.5317 - binary_accuracy: 0.7217 - val_loss: 0.7148 - val_binary_accuracy: 0.7107\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 116s - loss: 0.5309 - binary_accuracy: 0.7275 - val_loss: 0.6397 - val_binary_accuracy: 0.7251\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 116s - loss: 0.5221 - binary_accuracy: 0.7350 - val_loss: 0.5863 - val_binary_accuracy: 0.7447\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 116s - loss: 0.5153 - binary_accuracy: 0.7371 - val_loss: 0.6040 - val_binary_accuracy: 0.7282\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 119s - loss: 0.5154 - binary_accuracy: 0.7390 - val_loss: 0.6136 - val_binary_accuracy: 0.7277\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 118s - loss: 0.5079 - binary_accuracy: 0.7444 - val_loss: 0.8586 - val_binary_accuracy: 0.6921\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 124s - loss: 0.5059 - binary_accuracy: 0.7433 - val_loss: 1.2436 - val_binary_accuracy: 0.6343\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 116s - loss: 0.5139 - binary_accuracy: 0.7383 - val_loss: 0.8653 - val_binary_accuracy: 0.7143\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 116s - loss: 0.5006 - binary_accuracy: 0.7474 - val_loss: 0.8775 - val_binary_accuracy: 0.7205\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-bcf218809661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                     callbacks = [save_model, csv_logger, lr_scheduler])\n\u001b[0m",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2177\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1849\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_wide_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 32, 32, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_7 (GaussianNoise (None, 32, 32, 4)    0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 32, 32, 256)  50432       gaussian_noise_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 32, 32, 256)  1024        conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 32, 32, 256)  1638656     batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 32, 32, 256)  1024        conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 16, 16, 256)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 16, 16, 32)   524320      average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 16, 16, 32)   131104      average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 16, 16, 32)   128         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 16, 16, 32)   128         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 16, 16, 32)   16416       batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 16, 16, 32)   9248        batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 16, 16, 320)  0           average_pooling2d_10[0][0]       \n",
      "                                                                 conv2d_136[0][0]                 \n",
      "                                                                 conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 320)  0           concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 16, 16, 320)  1280        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 16, 16, 32)   92192       batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 16, 16, 32)   92192       batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 16, 16, 32)   128         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 16, 16, 32)   128         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 16, 16, 32)   9248        batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 16, 16, 32)   9248        batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 16, 16, 384)  0           batch_normalization_133[0][0]    \n",
      "                                                                 conv2d_140[0][0]                 \n",
      "                                                                 conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 384)  0           concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 16, 16, 384)  1536        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 8, 8, 384)    0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 8, 8, 32)     49184       average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 8, 8, 32)     196640      average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 8, 8, 32)     128         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 8, 8, 32)     128         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 8, 8, 32)     16416       batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 8, 8, 32)     9248        batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 448)    0           average_pooling2d_11[0][0]       \n",
      "                                                                 conv2d_144[0][0]                 \n",
      "                                                                 conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 448)    0           concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 8, 8, 448)    1792        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 8, 8, 32)     57376       batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 8, 8, 32)     57376       batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 8, 8, 32)     128         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 8, 8, 32)     128         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 8, 8, 32)     4128        batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 8, 8, 32)     4128        batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 512)    0           batch_normalization_139[0][0]    \n",
      "                                                                 conv2d_148[0][0]                 \n",
      "                                                                 conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 512)    0           concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 8, 8, 512)    2048        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 8, 8, 32)     147488      batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 8, 8, 32)     65568       batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 8, 8, 32)     128         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 8, 8, 32)     128         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 8, 8, 32)     4128        batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 8, 8, 32)     9248        batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 576)    0           batch_normalization_142[0][0]    \n",
      "                                                                 conv2d_152[0][0]                 \n",
      "                                                                 conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 576)    0           concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 8, 8, 576)    2304        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 4, 4, 576)    0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 9216)         0           average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1024)         9438208     flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1024)         0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1)            1025        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,645,505\n",
      "Trainable params: 12,639,361\n",
      "Non-trainable params: 6,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original implementation of Hilbert-CNN\n",
    "# https://openreview.net/forum?id=HJvvRoe0W\n",
    "\n",
    "def computation_block(in_layer, n_filters, filtersize_a, filtersize_b, filtersize_c, filtersize_d):\n",
    "\n",
    "    # residual 1\n",
    "    p1 = Conv2D(n_filters, (filtersize_a, filtersize_a), strides=(1, 1), padding='same')(in_layer)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "    p1 = Activation('relu')(p1)\n",
    "    p1 = Conv2D(n_filters, (filtersize_b, filtersize_b), strides=(1, 1), padding='same')(p1)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "\n",
    "    # residual 2\n",
    "    p2 = Conv2D(n_filters, (filtersize_c, filtersize_c), strides=(1, 1), padding='same')(in_layer)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "    p2 = Activation('relu')(p2)\n",
    "    p2 = Conv2D(n_filters, (filtersize_d, filtersize_d), strides=(1, 1), padding='same')(p2)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "\n",
    "    x = Concatenate()([in_layer, p1, p2])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# stem\n",
    "inputs = Input(shape=[32, 32, 4])\n",
    "x = GaussianNoise(0.2)(inputs)\n",
    "x = Conv2D(64, (7, 7), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (5, 5), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = computation_block(x, 4, 8, 4, 4, 3)\n",
    "x = computation_block(x, 4, 3, 3, 3, 3)\n",
    "# mid-stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = computation_block(x, 4, 2, 4, 4, 3)\n",
    "x = computation_block(x, 4, 2, 2, 2, 2)\n",
    "x = computation_block(x, 4, 3, 2, 2, 3)\n",
    "\n",
    "\n",
    "# exit stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "#x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x) #we omit this last avgpool to retain dimensionality\n",
    "x = Flatten()(x)\n",
    "\n",
    "# FC layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer= SGD(lr= 0.01, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55333, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/highdimdropout_hilbertcnn/weights-01-0.55.hdf5\n",
      " - 53s - loss: 0.5959 - binary_accuracy: 0.6777 - val_loss: 0.5533 - val_binary_accuracy: 0.7158\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 36s - loss: 0.5549 - binary_accuracy: 0.7085 - val_loss: 0.6000 - val_binary_accuracy: 0.6787\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55333 to 0.52281, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/highdimdropout_hilbertcnn/weights-03-0.52.hdf5\n",
      " - 36s - loss: 0.5368 - binary_accuracy: 0.7241 - val_loss: 0.5228 - val_binary_accuracy: 0.7463\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 36s - loss: 0.5326 - binary_accuracy: 0.7259 - val_loss: 0.5369 - val_binary_accuracy: 0.7179\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.52281 to 0.51251, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/highdimdropout_hilbertcnn/weights-05-0.51.hdf5\n",
      " - 37s - loss: 0.5185 - binary_accuracy: 0.7345 - val_loss: 0.5125 - val_binary_accuracy: 0.7189\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 36s - loss: 0.5142 - binary_accuracy: 0.7406 - val_loss: 0.5143 - val_binary_accuracy: 0.7292\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 36s - loss: 0.5097 - binary_accuracy: 0.7359 - val_loss: 0.5167 - val_binary_accuracy: 0.7354\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 36s - loss: 0.5014 - binary_accuracy: 0.7436 - val_loss: 0.5344 - val_binary_accuracy: 0.7298\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51251 to 0.49097, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/highdimdropout_hilbertcnn/weights-09-0.49.hdf5\n",
      " - 36s - loss: 0.5040 - binary_accuracy: 0.7457 - val_loss: 0.4910 - val_binary_accuracy: 0.7514\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 36s - loss: 0.4963 - binary_accuracy: 0.7469 - val_loss: 0.5069 - val_binary_accuracy: 0.7421\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 36s - loss: 0.4979 - binary_accuracy: 0.7467 - val_loss: 0.5048 - val_binary_accuracy: 0.7406\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 36s - loss: 0.4918 - binary_accuracy: 0.7507 - val_loss: 0.5139 - val_binary_accuracy: 0.7442\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 37s - loss: 0.4889 - binary_accuracy: 0.7524 - val_loss: 0.5113 - val_binary_accuracy: 0.7473\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 36s - loss: 0.4895 - binary_accuracy: 0.7510 - val_loss: 0.5050 - val_binary_accuracy: 0.7411\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 36s - loss: 0.4889 - binary_accuracy: 0.7535 - val_loss: 0.5324 - val_binary_accuracy: 0.7071\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 36s - loss: 0.4826 - binary_accuracy: 0.7582 - val_loss: 0.5048 - val_binary_accuracy: 0.7334\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 36s - loss: 0.4891 - binary_accuracy: 0.7532 - val_loss: 0.5066 - val_binary_accuracy: 0.7267\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 36s - loss: 0.4789 - binary_accuracy: 0.7627 - val_loss: 0.5098 - val_binary_accuracy: 0.7385\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 36s - loss: 0.4800 - binary_accuracy: 0.7582 - val_loss: 0.5067 - val_binary_accuracy: 0.7401\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 36s - loss: 0.4809 - binary_accuracy: 0.7588 - val_loss: 0.5156 - val_binary_accuracy: 0.7231\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 36s - loss: 0.4795 - binary_accuracy: 0.7572 - val_loss: 0.5304 - val_binary_accuracy: 0.7184\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 36s - loss: 0.4703 - binary_accuracy: 0.7621 - val_loss: 0.5275 - val_binary_accuracy: 0.7184\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-7c5c4d220891>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                     callbacks = [save_model, csv_logger, lr_scheduler])\n\u001b[0m",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2177\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1849\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/highdimdropout_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original implementation of Hilbert-CNN\n",
    "# https://openreview.net/forum?id=HJvvRoe0W\n",
    "\n",
    "def computation_block(in_layer, n_filters, filtersize_a, filtersize_b, filtersize_c, filtersize_d):\n",
    "\n",
    "    # residual 1\n",
    "    p1 = Conv2D(n_filters, (filtersize_a, filtersize_a), strides=(1, 1), padding='same')(in_layer)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "    p1 = Activation('relu')(p1)\n",
    "    p1 = Conv2D(n_filters, (filtersize_b, filtersize_b), strides=(1, 1), padding='same')(p1)\n",
    "    p1 = BatchNormalization()(p1)\n",
    "\n",
    "    # residual 2\n",
    "    p2 = Conv2D(n_filters, (filtersize_c, filtersize_c), strides=(1, 1), padding='same')(in_layer)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "    p2 = Activation('relu')(p2)\n",
    "    p2 = Conv2D(n_filters, (filtersize_d, filtersize_d), strides=(1, 1), padding='same')(p2)\n",
    "    p2 = BatchNormalization()(p2)\n",
    "\n",
    "    x = Concatenate()([in_layer, p1, p2])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# stem\n",
    "inputs = Input(shape=[32, 32, 4])\n",
    "x = GaussianNoise(0.2)(inputs)\n",
    "x = Conv2D(64, (7, 7), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (5, 5), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = computation_block(x, 4, 8, 4, 4, 3)\n",
    "x = computation_block(x, 4, 3, 3, 3, 3)\n",
    "# mid-stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = computation_block(x, 4, 2, 4, 4, 3)\n",
    "x = computation_block(x, 4, 2, 2, 2, 2)\n",
    "x = computation_block(x, 4, 3, 2, 2, 3)\n",
    "\n",
    "\n",
    "# exit stem\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x) \n",
    "x = Flatten()(x)\n",
    "\n",
    "# FC layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer= SGD(lr= 0.01, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54871, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/lowdimdropout_hilbertcnn/weights-01-0.55.hdf5\n",
      " - 55s - loss: 0.5959 - binary_accuracy: 0.6698 - val_loss: 0.5487 - val_binary_accuracy: 0.7133\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 36s - loss: 0.5505 - binary_accuracy: 0.7138 - val_loss: 0.5928 - val_binary_accuracy: 0.6699\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 36s - loss: 0.5381 - binary_accuracy: 0.7192 - val_loss: 0.5581 - val_binary_accuracy: 0.6978\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54871 to 0.53085, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/lowdimdropout_hilbertcnn/weights-04-0.53.hdf5\n",
      " - 36s - loss: 0.5316 - binary_accuracy: 0.7247 - val_loss: 0.5309 - val_binary_accuracy: 0.7153\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 36s - loss: 0.5239 - binary_accuracy: 0.7333 - val_loss: 0.5351 - val_binary_accuracy: 0.7236\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 36s - loss: 0.5169 - binary_accuracy: 0.7345 - val_loss: 0.5313 - val_binary_accuracy: 0.7313\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.53085 to 0.49480, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/lowdimdropout_hilbertcnn/weights-07-0.49.hdf5\n",
      " - 36s - loss: 0.5100 - binary_accuracy: 0.7427 - val_loss: 0.4948 - val_binary_accuracy: 0.7463\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 36s - loss: 0.5071 - binary_accuracy: 0.7446 - val_loss: 0.5142 - val_binary_accuracy: 0.7256\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 36s - loss: 0.5042 - binary_accuracy: 0.7451 - val_loss: 0.5175 - val_binary_accuracy: 0.7287\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 35s - loss: 0.5035 - binary_accuracy: 0.7496 - val_loss: 0.4980 - val_binary_accuracy: 0.7375\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 36s - loss: 0.4971 - binary_accuracy: 0.7484 - val_loss: 0.5025 - val_binary_accuracy: 0.7447\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 36s - loss: 0.4993 - binary_accuracy: 0.7494 - val_loss: 0.5188 - val_binary_accuracy: 0.7189\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 36s - loss: 0.4968 - binary_accuracy: 0.7461 - val_loss: 0.4975 - val_binary_accuracy: 0.7406\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 36s - loss: 0.4918 - binary_accuracy: 0.7529 - val_loss: 0.5031 - val_binary_accuracy: 0.7483\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 36s - loss: 0.4919 - binary_accuracy: 0.7530 - val_loss: 0.5126 - val_binary_accuracy: 0.7298\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 36s - loss: 0.4881 - binary_accuracy: 0.7551 - val_loss: 0.5069 - val_binary_accuracy: 0.7323\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 36s - loss: 0.4880 - binary_accuracy: 0.7571 - val_loss: 0.5034 - val_binary_accuracy: 0.7468\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 35s - loss: 0.4808 - binary_accuracy: 0.7586 - val_loss: 0.5277 - val_binary_accuracy: 0.7127\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.49480 to 0.49042, saving model to D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/lowdimdropout_hilbertcnn/weights-19-0.49.hdf5\n",
      " - 36s - loss: 0.4852 - binary_accuracy: 0.7571 - val_loss: 0.4904 - val_binary_accuracy: 0.7519\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 36s - loss: 0.4839 - binary_accuracy: 0.7563 - val_loss: 0.5160 - val_binary_accuracy: 0.7261\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 36s - loss: 0.4796 - binary_accuracy: 0.7576 - val_loss: 0.5165 - val_binary_accuracy: 0.7231\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 35s - loss: 0.4824 - binary_accuracy: 0.7604 - val_loss: 0.5012 - val_binary_accuracy: 0.7447\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 36s - loss: 0.4738 - binary_accuracy: 0.7626 - val_loss: 0.5112 - val_binary_accuracy: 0.7390\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 36s - loss: 0.4782 - binary_accuracy: 0.7617 - val_loss: 0.4955 - val_binary_accuracy: 0.7354\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 36s - loss: 0.4771 - binary_accuracy: 0.7594 - val_loss: 0.5049 - val_binary_accuracy: 0.7344\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 36s - loss: 0.4744 - binary_accuracy: 0.7651 - val_loss: 0.5135 - val_binary_accuracy: 0.7256\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 36s - loss: 0.4737 - binary_accuracy: 0.7652 - val_loss: 0.5072 - val_binary_accuracy: 0.7308\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 36s - loss: 0.4688 - binary_accuracy: 0.7652 - val_loss: 0.5058 - val_binary_accuracy: 0.7339\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 36s - loss: 0.4753 - binary_accuracy: 0.7644 - val_loss: 0.5086 - val_binary_accuracy: 0.7380\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 36s - loss: 0.4701 - binary_accuracy: 0.7652 - val_loss: 0.5148 - val_binary_accuracy: 0.7277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1878a000748>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/lowdimdropout_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_path = 'D:/Projects/iSynPro/iSynPro/HilbertCNN/train_val_npys/6count/1mer/test/high'\n",
    "low_path = 'D:/Projects/iSynPro/iSynPro/HilbertCNN/train_val_npys/6count/1mer/test/low'\n",
    "\n",
    "high_xcomp = []\n",
    "high_ycomp = []\n",
    "for root, subdir, files in os.walk(high_path):\n",
    "    for file in files:\n",
    "        high_xcomp.append(np.load(os.path.join(root, file)))\n",
    "        high_ycomp.append(0)\n",
    "\n",
    "low_xcomp = []\n",
    "low_ycomp = []\n",
    "for root, subdir, files in os.walk(low_path):\n",
    "    for file in files:\n",
    "        low_xcomp.append(np.load(os.path.join(root, file)))\n",
    "        low_ycomp.append(1)\n",
    "    \n",
    "x_test = np.asarray(low_xcomp + high_xcomp)\n",
    "y_test = np.asarray(low_ycomp + high_ycomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/vanilla_hilbertcnn/weights-18-0.50.hdf5',\n",
    "              'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_hilbertcnn/weights-16-0.50.hdf5',\n",
    "              'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_close_hilbertcnn/weights-07-0.50.hdf5',\n",
    "              'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/modified_wide_hilbertcnn/weights-04-0.55.hdf5',\n",
    "              'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/highdimdropout_hilbertcnn/weights-09-0.49.hdf5',\n",
    "              'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/lowdimdropout_hilbertcnn/weights-19-0.49.hdf5'\n",
    "             ]\n",
    "label_list = ['Vanilla Hilbert-CNN',\n",
    "              'Batchnorm -BlockRelu +FinalRelu',\n",
    "              'Batchnorm +BlockRelu -FinalRelu',\n",
    "              'Wide Hilbert-CNN',\n",
    "              'Hilbert-CNN -FinalMaxPool +Dropout',\n",
    "              'Hilbert-CNN +FinalMaxPool +Dropout']\n",
    "roc_list = []\n",
    "for path in model_list:\n",
    "    model = load_model(path)\n",
    "    y_pred = model.predict(x_test)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    roc_list.append([fpr, tpr, auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAE0oANCEAgDtAzARZRzm5vp4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187e52700b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "palette = sns.color_palette(\"cubehelix\", len(roc_list))\n",
    "\n",
    "#plot roc curve\n",
    "for i in range(len(roc_list)):\n",
    "    plt.plot(roc_list[i][0], \n",
    "             roc_list[i][1], \n",
    "             color=palette[i], \n",
    "             label='{0} (AUC = {1:.3f})'.format(label_list[i], roc_list[i][2]))\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig('c:/users/wolf/desktop/SynPro/roc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications import mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wolfgang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\applications\\imagenet_utils.py:257: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 4 input channels.\n",
      "  str(input_shape[-1]) + ' input channels.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 16, 16, 32)        1152      \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 16, 16, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 16, 16, 64)        2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 8, 8, 64)          576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 8, 8, 128)         8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 8, 8, 128)         1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 8, 8, 128)         16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 4, 4, 128)         1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 4, 4, 256)         32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 4, 4, 256)         2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 4, 4, 256)         65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 2, 2, 256)         2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 2, 2, 512)         131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 2, 2, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 2, 2, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 2, 2, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 2, 2, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 2, 2, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 2, 2, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 2, 2, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 2, 2, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 2, 2, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 2, 2, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 1, 1, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 1, 1, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 1, 1, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 1, 1, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 1, 1, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 1, 1, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 1, 1, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 1, 1, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 4,279,777\n",
      "Trainable params: 4,257,889\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "head_model = MobileNet(include_top=False,\n",
    "                       weights=None,\n",
    "                       input_shape = (32, 32, 4))\n",
    "\n",
    "x = head_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=head_model.input, outputs=predictions)\n",
    "model.compile(optimizer= SGD(lr= 0.01, momentum=0.9),\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=[ 'binary_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "train_size = 17588\n",
    "test_size = 1955\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.94\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#our callbacks\n",
    "root_path = 'D:/Projects/Github/SyntheticPromoter/HilbertCNN/weights/1mer/mobile_hilbertcnn'\n",
    "save_model = ModelCheckpoint(root_path + '/weights-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             monitor='val_loss',\n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto',\n",
    "                                             period=1)\n",
    "\n",
    "csv_path = '{}/training_history.csv'.format(root_path)\n",
    "csv_logger = CSVLogger(csv_path, separator=',', append=False)\n",
    "\n",
    "def incep_resnet_schedule(epoch):\n",
    "    if epoch % 2 == 0:\n",
    "        return learning_rate*(learning_decay**(epoch))\n",
    "    else:\n",
    "        return learning_rate*(learning_decay**((epoch)-1.0))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(incep_resnet_schedule)\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= train_size // batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data= validation_generator,\n",
    "                    validation_steps= test_size // batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks = [save_model, csv_logger, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
